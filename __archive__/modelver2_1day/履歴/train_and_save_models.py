import pandas as pd
import numpy as np
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    GradientBoostingClassifier,
)
from sklearn.model_selection import train_test_split, KFold
from sklearn.base import clone
import joblib
import os
from sqlalchemy import create_engine, text
from utils.get_holydays import get_japanese_holidays
from logic.factory_managesql import get_training_date_range, load_data_from_sqlite
from utils.config_loader import get_path_from_yaml


# ===============================
# ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨ä¿å­˜å‡¦ç†
# ===============================
def train_and_save_models(
    df_raw: pd.DataFrame, holidays: list[str], save_dir: str = "models"
):
    os.makedirs(save_dir, exist_ok=True)

    # --- ãƒ‡ãƒ¼ã‚¿åŠ å·¥ï¼ˆãƒ”ãƒœãƒƒãƒˆï¼‰ ---
    df_pivot = (
        df_raw.groupby(["ä¼ç¥¨æ—¥ä»˜", "å“å"])["æ­£å‘³é‡é‡"].sum().unstack(fill_value=0)
    )
    df_pivot["åˆè¨ˆ"] = df_pivot.sum(axis=1)

    # --- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    df_feat = pd.DataFrame(index=df_pivot.index)
    df_feat["æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥"] = df_pivot["æ··åˆå»ƒæ£„ç‰©A"].shift(1)
    df_feat["æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥"] = df_pivot["æ··åˆå»ƒæ£„ç‰©B"].shift(1)
    df_feat["åˆè¨ˆ_å‰æ—¥"] = df_pivot["åˆè¨ˆ"].shift(1)
    df_feat["åˆè¨ˆ_3æ—¥å¹³å‡"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).mean()
    df_feat["åˆè¨ˆ_3æ—¥åˆè¨ˆ"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).sum()
    df_feat["æ›œæ—¥"] = df_feat.index.dayofweek
    df_feat["é€±ç•ªå·"] = df_feat.index.isocalendar().week

    # ä¸­å¤®å€¤ã‚’ç”¨ã„ãŸå®‰å®šçš„ç‰¹å¾´é‡
    daily_avg = df_raw.groupby("ä¼ç¥¨æ—¥ä»˜")["æ­£å‘³é‡é‡"].median()
    df_feat["1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤"] = daily_avg.shift(1).expanding().median()

    # ç¥æ—¥ãƒ•ãƒ©ã‚°
    holiday_dates = pd.to_datetime(holidays)
    df_feat["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df_feat.index.isin(holiday_dates).astype(int)

    # æ¬ æé™¤å»
    df_feat = df_feat.dropna()
    df_pivot = df_pivot.loc[df_feat.index]

    # --- ç‰¹å¾´é‡ãƒ»å¯¾è±¡å“ç›®ã®å®šç¾© ---
    ab_features = [
        "æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥",
        "æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥",
        "åˆè¨ˆ_å‰æ—¥",
        "åˆè¨ˆ_3æ—¥å¹³å‡",
        "åˆè¨ˆ_3æ—¥åˆè¨ˆ",
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
    ]
    target_items = ["æ··åˆå»ƒæ£„ç‰©A", "æ··åˆå»ƒæ£„ç‰©B", "æ··åˆå»ƒæ£„ç‰©(ï½¿ï¾Œï½§ï½°ï½¥å®¶å…·é¡)"]

    # --- å„ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ ---
    base_models = [
        ("elastic", ElasticNet(alpha=0.1, l1_ratio=0.5)),
        ("rf", RandomForestRegressor(n_estimators=100, random_state=42)),
    ]
    meta_model_stage1 = ElasticNet(alpha=0.1, l1_ratio=0.5)
    gbdt_model = GradientBoostingRegressor(
        n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42
    )

    # --- ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°å­¦ç¿’ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸1ï¼‰ ---
    X_features_all = {}
    stacked_preds = {}
    kf = KFold(n_splits=5)

    for item in target_items:
        X = (
            df_feat[ab_features]
            if item == "æ··åˆå»ƒæ£„ç‰©A"
            else df_feat[[c for c in ab_features if "1å°ã‚ãŸã‚Š" not in c]]
        )
        y = df_pivot[item]
        X_features_all[item] = X

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )

        train_meta = np.zeros((X_train.shape[0], len(base_models)))
        for i, (_, model) in enumerate(base_models):
            for train_idx, val_idx in kf.split(X_train):
                model_ = clone(model)
                model_.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])
                train_meta[val_idx, i] = model_.predict(X_train.iloc[val_idx])
        meta_model_stage1.fit(train_meta, y_train)

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸2ç”¨å…¥åŠ›ï¼‰
        test_meta = np.column_stack(
            [
                clone(model).fit(X_train, y_train).predict(X_test)
                for _, model in base_models
            ]
        )
        stacked_preds[item] = meta_model_stage1.predict(test_meta)

    # --- ã‚¹ãƒ†ãƒ¼ã‚¸2å…¥åŠ›ã®æ§‹ç¯‰ ---
    index_final = X_test.index
    df_stage1 = pd.DataFrame(
        {f"{k}_äºˆæ¸¬": v for k, v in stacked_preds.items()}, index=index_final
    )
    for col in [
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "åˆè¨ˆ_å‰æ—¥",
        "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
    ]:
        df_stage1[col] = df_feat.loc[index_final, col]

    # --- ã‚¹ãƒ†ãƒ¼ã‚¸2: åˆè¨ˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ« ---
    y_total_final = df_pivot.loc[df_stage1.index, "åˆè¨ˆ"]
    gbdt_model.fit(df_stage1, y_total_final)

    # --- ã‚¹ãƒ†ãƒ¼ã‚¸2: åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆè­¦å‘Šåˆ¤å®šï¼‰ ---
    y_total_binary = (y_total_final < 90000).astype(int)
    clf_model = GradientBoostingClassifier(
        n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42
    )
    clf_model.fit(df_stage1.drop(columns=["ç¥æ—¥ãƒ•ãƒ©ã‚°"]), y_total_binary)

    # --- ãƒ¢ãƒ‡ãƒ«ä¿å­˜ ---
    joblib.dump(meta_model_stage1, f"{save_dir}/meta_model_stage1.pkl")
    joblib.dump(gbdt_model, f"{save_dir}/gbdt_model_stage2.pkl")
    joblib.dump(clf_model, f"{save_dir}/clf_model.pkl")
    joblib.dump(ab_features, f"{save_dir}/ab_features.pkl")
    joblib.dump(X_features_all, f"{save_dir}/X_features_all.pkl")
    joblib.dump(df_feat, f"{save_dir}/df_feat.pkl")
    joblib.dump(df_pivot, f"{save_dir}/df_pivot.pkl")

    print(f"âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼†ä¿å­˜å®Œäº† â†’ {save_dir}/ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")


# ===============================
# ğŸš€ ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
# ===============================
def create_model():
    # SQLã®è¨­å®š
    sql_url = get_path_from_yaml("weight_data", section="sql_database")
    print(sql_url)
    table_name = "ukeire"

    # --- ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®è¨­å®š ---
    model_path = get_path_from_yaml(
        ["models", "predicted_import_volume"], section="directories"
    )
    # print(model_path)
    # ãƒ‡ãƒ¼ã‚¿èª­è¾¼ã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»ä¿å­˜
    df_raw = load_data_from_sqlite()
    holiday_dates = df_raw.loc[df_raw["ç¥æ—¥ãƒ•ãƒ©ã‚°"] == 1, "ä¼ç¥¨æ—¥ä»˜"].unique()
    df_raw.drop(columns=["ç¥æ—¥ãƒ•ãƒ©ã‚°"], inplace=True)
    # print(len(df_raw))
    # print(df_raw.head(50))
    train_and_save_models(df_raw=df_raw, holidays=holiday_dates, save_dir=model_path)
    return


# ===============================
# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ===============================
if __name__ == "__main__":
    import time

    start_time = time.time()  # é–‹å§‹æ™‚é–“
    print("ãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹")
    create_model()

    end_time = time.time()  # çµ‚äº†æ™‚é–“
    elapsed_time = end_time - start_time
    print(f"â±ï¸ å‡¦ç†æ™‚é–“: {elapsed_time:.2f} ç§’")
