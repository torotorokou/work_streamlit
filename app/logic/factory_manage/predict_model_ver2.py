import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge, LogisticRegression, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.base import clone
from sklearn.metrics import accuracy_score, roc_auc_score
import joblib
from utils.config_loader import get_path_from_yaml
import pandas as pd
import numpy as np
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    GradientBoostingClassifier,
)
from sklearn.model_selection import train_test_split, KFold
from sklearn.base import clone
from sklearn.metrics import r2_score, mean_absolute_error
from utils.get_holydays import get_japanese_holidays
from utils.config_loader import get_path_from_yaml
from logic.factory_manage.sql import load_data_from_sqlite

# data_prep.py
import pandas as pd
import pandas as pd
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    GradientBoostingClassifier,
)
from sklearn.model_selection import TimeSeriesSplit


def prepare_features(
    df_raw: pd.DataFrame, holidays: list[str]
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆdf_pivotï¼‰ã‚’ä½œæˆã™ã‚‹

    Args:
        df_raw (pd.DataFrame): å…ƒãƒ‡ãƒ¼ã‚¿
        holidays (list[str]): ç¥æ—¥ãƒªã‚¹ãƒˆ

    Returns:
        tuple:
            - df_feat (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            - df_pivot (pd.DataFrame): å“ç›®åˆ¥æ—¥åˆ¥åˆè¨ˆãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆå«ã‚€ï¼‰
    """
    # --- ãƒ”ãƒœãƒƒãƒˆä½œæˆï¼ˆå“ç›®åˆ¥åˆè¨ˆï¼‰ ---
    df_pivot = (
        df_raw.groupby(["ä¼ç¥¨æ—¥ä»˜", "å“å"])["æ­£å‘³é‡é‡"].sum().unstack(fill_value=0)
    )
    df_pivot["åˆè¨ˆ"] = df_pivot.sum(axis=1)

    # --- ç‰¹å¾´é‡ä½œæˆ ---
    df_feat = pd.DataFrame(index=df_pivot.index)
    df_feat["æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥"] = df_pivot["æ··åˆå»ƒæ£„ç‰©A"].shift(1)
    df_feat["æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥"] = df_pivot["æ··åˆå»ƒæ£„ç‰©B"].shift(1)
    df_feat["åˆè¨ˆ_å‰æ—¥"] = df_pivot["åˆè¨ˆ"].shift(1)
    df_feat["åˆè¨ˆ_3æ—¥å¹³å‡"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).mean()
    df_feat["åˆè¨ˆ_3æ—¥åˆè¨ˆ"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).sum()
    df_feat["æ›œæ—¥"] = df_feat.index.dayofweek
    df_feat["é€±ç•ªå·"] = df_feat.index.isocalendar().week

    # --- 1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡ã®å‰æ—¥ä¸­å¤®å€¤ï¼ˆç´¯ç©ï¼‰ ---
    daily_avg = df_raw.groupby("ä¼ç¥¨æ—¥ä»˜")["æ­£å‘³é‡é‡"].median()
    df_feat["1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤"] = daily_avg.shift(1).expanding().median()

    # --- ç¥æ—¥ãƒ•ãƒ©ã‚° ---
    holiday_dates = pd.to_datetime(holidays)
    df_feat["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df_feat.index.isin(holiday_dates).astype(int)

    # --- æ¬ æé™¤å» ---
    df_feat = df_feat.dropna()
    df_pivot = df_pivot.loc[df_feat.index]

    return df_feat, df_pivot


# model_stage1.py
from sklearn.base import clone
import numpy as np


def train_stage1_models(
    df_feat, df_pivot, tscv, base_models, meta_model_stage1, ab_features, target_items
):
    """
    ã‚¹ãƒ†ãƒ¼ã‚¸1ã®å­¦ç¿’ï¼ˆã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ç”¨ãƒ¡ã‚¿ç‰¹å¾´é‡ç”Ÿæˆï¼‰

    Args:
        df_feat (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        df_pivot (pd.DataFrame): å“ç›®åˆ¥åˆè¨ˆãƒ‡ãƒ¼ã‚¿
        tscv (TimeSeriesSplit): æ™‚ç³»åˆ—CV
        base_models (list): ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        meta_model_stage1: ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆElasticNetï¼‰
        ab_features (list): ä½¿ç”¨ç‰¹å¾´é‡å
        target_items (list): å¯¾è±¡å“ç›®

    Returns:
        tuple:
            - X_features_all (dict): å„å“ç›®ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            - stacked_preds (dict): å„å“ç›®ã®ãƒ†ã‚¹ãƒˆäºˆæ¸¬å€¤
    """
    X_features_all = {}
    stacked_preds = {}

    for item in target_items:
        # --- å“ç›®ã”ã¨ã«ç‰¹å¾´é‡ã‚’é¸æŠ ---
        X = (
            df_feat[ab_features]
            if item == "æ··åˆå»ƒæ£„ç‰©A"
            else df_feat[[c for c in ab_features if "1å°ã‚ãŸã‚Š" not in c]]
        )
        y = df_pivot[item]

        # --- æ™‚ç³»åˆ—ã®æœ€å¾Œ20%ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ ---
        test_size = int(len(X) * 0.2)
        X_train = X.iloc[:-test_size]
        X_test = X.iloc[-test_size:]
        y_train = y.iloc[:-test_size]
        y_test = y.iloc[-test_size:]

        X_features_all[item] = X

        # --- ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ç”¨ãƒ¡ã‚¿ç‰¹å¾´é‡ä½œæˆ ---
        train_meta = np.zeros((X_train.shape[0], len(base_models)))
        for i, (_, model) in enumerate(base_models):
            for train_idx, val_idx in tscv.split(X_train):
                model_ = clone(model)
                model_.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])
                train_meta[val_idx, i] = model_.predict(X_train.iloc[val_idx])

        # --- ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ---
        meta_model_stage1.fit(train_meta, y_train)

        # --- ãƒ†ã‚¹ãƒˆäºˆæ¸¬ ---
        test_meta = np.column_stack(
            [
                clone(model).fit(X_train, y_train).predict(X_test)
                for _, model in base_models
            ]
        )
        stacked_preds[item] = meta_model_stage1.predict(test_meta)

    return X_features_all, stacked_preds


# model_stage2.py
from sklearn.metrics import r2_score, mean_absolute_error


def train_stage2_models(df_stage1, df_pivot, gbdt_model, clf_model):
    """
    ã‚¹ãƒ†ãƒ¼ã‚¸2ãƒ¢ãƒ‡ãƒ«ï¼ˆGBDTå›å¸° + åˆ†é¡å™¨ï¼‰å­¦ç¿’

    Args:
        df_stage1 (pd.DataFrame): ã‚¹ãƒ†ãƒ¼ã‚¸1ã®å‡ºåŠ›ç‰¹å¾´é‡
        df_pivot (pd.DataFrame): å“ç›®åˆ¥åˆè¨ˆãƒ‡ãƒ¼ã‚¿
        gbdt_model: å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆGBDTï¼‰
        clf_model: åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆGBCï¼‰

    Returns:
        tuple:
            - gbdt_model: å­¦ç¿’æ¸ˆã¿å›å¸°ãƒ¢ãƒ‡ãƒ«
            - clf_model: å­¦ç¿’æ¸ˆã¿åˆ†é¡ãƒ¢ãƒ‡ãƒ«
            - r2 (float): RÂ²ã‚¹ã‚³ã‚¢
            - mae (float): MAE
    """
    # --- å›å¸°å­¦ç¿’ ---
    y_total_final = df_pivot.loc[df_stage1.index, "åˆè¨ˆ"]
    gbdt_model.fit(df_stage1, y_total_final)

    # --- åˆ†é¡å­¦ç¿’ï¼ˆ90000æœªæº€ or ä»¥ä¸Šï¼‰ ---
    y_total_binary = (y_total_final < 90000).astype(int)
    clf_model.fit(df_stage1.drop(columns=["ç¥æ—¥ãƒ•ãƒ©ã‚°"]), y_total_binary)

    # --- è©•ä¾¡ ---
    r2 = r2_score(y_total_final, gbdt_model.predict(df_stage1))
    mae = mean_absolute_error(y_total_final, gbdt_model.predict(df_stage1))

    return gbdt_model, clf_model, r2, mae


# predict.py
import numpy as np
import pandas as pd
from sklearn.base import clone


def predict_future(
    df_feat,
    df_pivot,
    df_stage1,
    X_features_all,
    meta_model_stage1,
    gbdt_model,
    clf_model,
    base_models,
    target_items,
    ab_features,
    start_date,
    end_date,
    holidays,
):
    """
    å°†æ¥æœŸé–“ã«å¯¾ã™ã‚‹äºˆæ¸¬ï¼ˆå›å¸° + åˆ†é¡ãƒ©ãƒ™ãƒ«ä»˜ã‘ï¼‰

    Args:
        df_feat (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        df_pivot (pd.DataFrame): å“ç›®åˆ¥åˆè¨ˆãƒ‡ãƒ¼ã‚¿
        df_stage1 (pd.DataFrame): ã‚¹ãƒ†ãƒ¼ã‚¸1ã®å‡ºåŠ›ç‰¹å¾´é‡
        X_features_all (dict): å„å“ç›®ã®ç‰¹å¾´é‡
        meta_model_stage1: ã‚¹ãƒ†ãƒ¼ã‚¸1ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
        gbdt_model: å­¦ç¿’æ¸ˆã¿å›å¸°ãƒ¢ãƒ‡ãƒ«
        clf_model: å­¦ç¿’æ¸ˆã¿åˆ†é¡ãƒ¢ãƒ‡ãƒ«
        base_models (list): ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        target_items (list): å¯¾è±¡å“ç›®
        ab_features (list): ä½¿ç”¨ç‰¹å¾´é‡å
        start_date (str): äºˆæ¸¬é–‹å§‹æ—¥
        end_date (str): äºˆæ¸¬çµ‚äº†æ—¥
        holidays (list): ç¥æ—¥ãƒªã‚¹ãƒˆ

    Returns:
        pd.DataFrame: äºˆæ¸¬çµæœ
    """
    holiday_dates = pd.to_datetime(holidays)
    last_date = df_feat.index[-1]
    predict_dates = pd.date_range(start=start_date, end=end_date)

    # --- ãƒã‚¤ã‚¢ã‚¹ãƒ»æ¨™æº–åå·®è¨ˆç®— ---
    y_total_final = df_pivot.loc[df_stage1.index, "åˆè¨ˆ"]
    residuals = y_total_final - gbdt_model.predict(df_stage1)
    bias = residuals.mean()
    std = residuals.std()

    results = []
    for predict_date in predict_dates:
        # --- 1æ—¥ã®ç‰¹å¾´é‡ç”Ÿæˆ ---
        new_row = {
            "æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥": df_pivot.loc[last_date, "æ··åˆå»ƒæ£„ç‰©A"],
            "æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥": df_pivot.loc[last_date, "æ··åˆå»ƒæ£„ç‰©B"],
            "åˆè¨ˆ_å‰æ—¥": df_pivot.loc[last_date, "åˆè¨ˆ"],
            "åˆè¨ˆ_3æ—¥å¹³å‡": df_pivot["åˆè¨ˆ"].shift(1).rolling(3).mean().loc[last_date],
            "åˆè¨ˆ_3æ—¥åˆè¨ˆ": df_pivot["åˆè¨ˆ"].shift(1).rolling(3).sum().loc[last_date],
            "æ›œæ—¥": predict_date.dayofweek,
            "é€±ç•ªå·": predict_date.isocalendar().week,
            "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤": df_feat[
                "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤"
            ].iloc[-1],
            "ç¥æ—¥ãƒ•ãƒ©ã‚°": int(predict_date in holiday_dates),
        }
        df_input = pd.DataFrame(new_row, index=[predict_date])

        # --- ã‚¹ãƒ†ãƒ¼ã‚¸1äºˆæ¸¬ ---
        for item in target_items:
            x_item = (
                df_input[ab_features]
                if item == "æ··åˆå»ƒæ£„ç‰©A"
                else df_input[[c for c in ab_features if "1å°ã‚ãŸã‚Š" not in c]]
            )
            meta_input = np.column_stack(
                [
                    clone(model)
                    .fit(X_features_all[item], df_pivot.loc[df_feat.index, item])
                    .predict(x_item)
                    for _, model in base_models
                ]
            )
            df_input[f"{item}_äºˆæ¸¬"] = meta_model_stage1.predict(meta_input)[0]

        # --- ã‚¹ãƒ†ãƒ¼ã‚¸2äºˆæ¸¬ ---
        stage2_input = df_input[
            [
                f"{target_items[0]}_äºˆæ¸¬",
                f"{target_items[1]}_äºˆæ¸¬",
                f"{target_items[2]}_äºˆæ¸¬",
                "æ›œæ—¥",
                "é€±ç•ªå·",
                "åˆè¨ˆ_å‰æ—¥",
                "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
                "ç¥æ—¥ãƒ•ãƒ©ã‚°",
            ]
        ]
        y_pred = gbdt_model.predict(stage2_input)[0]
        y_adjusted = y_pred + bias
        lower = y_adjusted - 1.96 * std
        upper = y_adjusted + 1.96 * std

        # --- åˆ¤å®šãƒ©ãƒ™ãƒ« ---
        label = "é€šå¸¸"
        prob = None
        if 85000 <= y_adjusted <= 95000:
            X_clf = stage2_input.drop(columns=["ç¥æ—¥ãƒ•ãƒ©ã‚°"])
            prob = clf_model.predict_proba(X_clf)[0][1]
            classification = clf_model.predict(X_clf)[0]
            label = "è­¦å‘Š" if classification == 1 else "æ³¨æ„"

        # --- çµæœæ ¼ç´ ---
        results.append(
            {
                "æ—¥ä»˜": predict_date.strftime("%Y-%m-%d"),
                "äºˆæ¸¬å€¤": y_pred,
                "è£œæ­£å¾Œäºˆæ¸¬": y_adjusted,
                "ä¸‹é™95CI": lower,
                "ä¸Šé™95CI": upper,
                "åˆ¤å®šãƒ©ãƒ™ãƒ«": label,
                "æœªæº€ç¢ºç‡": round(prob, 3) if prob is not None else None,
            }
        )

    df_result = pd.DataFrame(results).set_index("æ—¥ä»˜")
    return df_result


def get_df():
    base_dir = get_path_from_yaml("input", section="directories")

    df_raw = pd.read_csv(f"{base_dir}/20240501-20250422.csv", encoding="utf-8")
    df_raw = df_raw[["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡", "å“å"]]
    df2 = pd.read_csv(f"{base_dir}/2020é¡§å®¢.csv")
    df3 = pd.read_csv(f"{base_dir}/2021é¡§å®¢.csv")
    df4 = pd.read_csv(f"{base_dir}/2023_all.csv")

    df2 = df2[["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]]
    df3 = df3[["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]]
    df4 = df4[["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]]

    df_all = pd.concat([df2, df3, df4])
    df_all["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_all["ä¼ç¥¨æ—¥ä»˜"])

    df_all.rename(columns={"å•†å“": "å“å", "æ­£å‘³é‡é‡_æ˜ç´°": "æ­£å‘³é‡é‡"}, inplace=True)

    df_raw = pd.concat([df_raw, df_all])
    df_raw = df_raw.copy()
    df_raw["ä¼ç¥¨æ—¥ä»˜"] = df_raw["ä¼ç¥¨æ—¥ä»˜"].str.replace(r"\(.*\)", "", regex=True)
    df_raw["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_raw["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")
    df_raw["æ­£å‘³é‡é‡"] = pd.to_numeric(df_raw["æ­£å‘³é‡é‡"], errors="coerce")
    df_raw = df_raw.dropna(subset=["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"])
    return df_raw


def get_date_holidays(df):
    """
    dfå†…ã®ç¥æ—¥ãƒ•ãƒ©ã‚°=1ã®æ—¥ä»˜ã‚’ä¸€æ„ã«å–å¾—ã—ã€start_dateï½end_dateã®ç¯„å›²å†…ã§è¿”ã™

    Args:
        df (pd.DataFrame): ãƒ‡ãƒ¼ã‚¿ï¼ˆ'ä¼ç¥¨æ—¥ä»˜'ã€'ç¥æ—¥ãƒ•ãƒ©ã‚°'ã‚«ãƒ©ãƒ ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ï¼‰

    Returns:
        list[str]: ç¥æ—¥ã®æ—¥ä»˜ï¼ˆYYYY-MM-DD å½¢å¼ï¼‰ã®ãƒªã‚¹ãƒˆ
    """

    start_date = df["ä¼ç¥¨æ—¥ä»˜"].min().date()
    end_date = df["ä¼ç¥¨æ—¥ä»˜"].max().date()

    # print(f"ğŸ” ç¥æ—¥æŠ½å‡ºç¯„å›²: {start_date} ï½ {end_date}")

    # --- ç¥æ—¥ãƒ•ãƒ©ã‚°ãŒ1ã®è¡Œã®ã¿æŠ½å‡º ---
    mask = df["ç¥æ—¥ãƒ•ãƒ©ã‚°"] == 1
    holidays_series = df.loc[mask, "ä¼ç¥¨æ—¥ä»˜"]

    # --- é‡è¤‡é™¤å» & æ—¥ä»˜ç¯„å›²å†…ã§çµã‚Šè¾¼ã¿ ---
    holidays = holidays_series.drop_duplicates()
    holidays = holidays[
        (holidays.dt.date >= start_date) & (holidays.dt.date <= end_date)
    ]

    # --- æ—¥ä»˜å‹ã‚’æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰ã«å¤‰æ›ã—ã¦ãƒªã‚¹ãƒˆåŒ– ---
    holidays_list = holidays.dt.strftime("%Y-%m-%d").tolist()

    return holidays_list


def debug(df):
    """
    ãƒ‡ãƒãƒƒã‚°ç”¨ã®é–¢æ•°ã€‚DataFrameã®åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        df (pd.DataFrame): ãƒ‡ãƒãƒƒã‚°å¯¾è±¡ã®DataFrame
    """
    df = df["ä¼ç¥¨æ—¥ä»˜"].unique()
    for i in range(len(df)):
        print(df[i])
    return None


def predict_controller(start_date, end_date):
    # --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
    df_raw = load_data_from_sqlite()

    # df_rawã‹ã‚‰holidaysã‚’å–å¾—&æ•´å½¢
    holidays = get_date_holidays(df_raw)
    df_raw = df_raw[["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡", "å“å"]].copy()

    # ãƒ‡ãƒãƒƒã‚°ç”¨
    # debug(get_df())

    # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
    target_items = ["æ··åˆå»ƒæ£„ç‰©A", "æ··åˆå»ƒæ£„ç‰©B", "æ··åˆå»ƒæ£„ç‰©(ï½¿ï¾Œï½§ï½°ï½¥å®¶å…·é¡)"]
    ab_features = [
        "æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥",
        "æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥",
        "åˆè¨ˆ_å‰æ—¥",
        "åˆè¨ˆ_3æ—¥å¹³å‡",
        "åˆè¨ˆ_3æ—¥åˆè¨ˆ",
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
    ]
    base_models = [
        ("elastic", ElasticNet(alpha=0.1, l1_ratio=0.5)),
        ("rf", RandomForestRegressor(n_estimators=100, random_state=42)),
    ]
    meta_model_stage1 = ElasticNet(alpha=0.1, l1_ratio=0.5)
    gbdt_model = GradientBoostingRegressor(
        n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42
    )
    clf_model = GradientBoostingClassifier(
        n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42
    )
    tscv = TimeSeriesSplit(n_splits=5)

    # --- ç‰¹å¾´é‡ä½œæˆ ---
    df_feat, df_pivot = prepare_features(df_raw, holidays)

    # --- ã‚¹ãƒ†ãƒ¼ã‚¸1å­¦ç¿’ ---
    X_features_all, stacked_preds = train_stage1_models(
        df_feat,
        df_pivot,
        tscv,
        base_models,
        meta_model_stage1,
        ab_features,
        target_items,
    )

    # --- ã‚¹ãƒ†ãƒ¼ã‚¸2å­¦ç¿’ ---
    index_final = df_feat.iloc[int(len(df_feat) * 0.8) :].index
    df_stage1 = pd.DataFrame(
        {f"{k}_äºˆæ¸¬": v for k, v in stacked_preds.items()}, index=index_final
    )
    for col in [
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "åˆè¨ˆ_å‰æ—¥",
        "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
    ]:
        df_stage1[col] = df_feat.loc[index_final, col]

    gbdt_model, clf_model, r2, mae = train_stage2_models(
        df_stage1, df_pivot, gbdt_model, clf_model
    )
    print(f"âœ… RÂ² = {r2:.3f}, MAE = {mae:,.0f} kg")

    # --- å°†æ¥äºˆæ¸¬ ---
    df_result = predict_future(
        df_feat,
        df_pivot,
        df_stage1,  # è¿½åŠ æ¸ˆã¿OK
        X_features_all,
        meta_model_stage1,
        gbdt_model,
        clf_model,
        base_models,
        target_items,
        ab_features,  # â† ã“ã‚Œã‚’å¿˜ã‚Œãšã«å…¥ã‚Œã‚‹
        start_date=start_date,
        end_date=end_date,
        holidays=holidays,
    )

    return df_result


if __name__ == "__main__":
    df_result = predict_controller("2025-06-01", "2025-06-07")
    print(df_result)
