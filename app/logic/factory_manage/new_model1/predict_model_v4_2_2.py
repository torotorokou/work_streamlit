import pandas as pd
import numpy as np
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.base import clone
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold


def get_target_items(df_raw, top_n=5):
    return df_raw["å“å"].value_counts().head(top_n).index.tolist()


def generate_reserve_features(df_reserve, top_k_clients=10):
    df_reserve = df_reserve.copy()
    df_reserve["äºˆç´„æ—¥"] = pd.to_datetime(df_reserve["äºˆç´„æ—¥"])

    # ä¸Šä½å¾—æ„å…ˆãƒ•ãƒ©ã‚°ã‚’ä½œæˆ
    top_clients = df_reserve["äºˆç´„å¾—æ„å…ˆå"].value_counts().head(top_k_clients).index
    df_reserve["ä¸Šä½å¾—æ„å…ˆãƒ•ãƒ©ã‚°"] = (
        df_reserve["äºˆç´„å¾—æ„å…ˆå"].isin(top_clients).astype(int)
    )

    # å°æ•°ï¼ˆæ•°å€¤ï¼‰ã«å¤‰æ›ã—ã¦ãŠãï¼ˆå¿µã®ãŸã‚ï¼‰
    df_reserve["å°æ•°"] = pd.to_numeric(df_reserve["å°æ•°"], errors="coerce").fillna(0)

    # é›†è¨ˆå‡¦ç†
    df_feat = df_reserve.groupby("äºˆç´„æ—¥").agg(
        äºˆç´„ä»¶æ•°=("äºˆç´„å¾—æ„å…ˆå", "count"),
        å›ºå®šå®¢äºˆç´„æ•°=("å›ºå®šå®¢", lambda x: x.sum()),
        éå›ºå®šå®¢äºˆç´„æ•°=("å›ºå®šå®¢", lambda x: (~x).sum()),
        ä¸Šä½å¾—æ„å…ˆäºˆç´„æ•°=("ä¸Šä½å¾—æ„å…ˆãƒ•ãƒ©ã‚°", "sum"),
        äºˆç´„åˆè¨ˆå°æ•°=("å°æ•°", "sum"),  # â†â˜… è¿½åŠ ï¼šäºˆç´„æ—¥ã”ã¨ã®å°æ•°åˆè¨ˆ
        å¹³å‡å°æ•°=("å°æ•°", "mean"),  # â†ï¼ˆä»»æ„ï¼‰1ä»¶ã‚ãŸã‚Šã®å°æ•°ã‚‚æ¬²ã—ã„å ´åˆ
    )
    df_feat["å›ºå®šå®¢æ¯”ç‡"] = df_feat["å›ºå®šå®¢äºˆç´„æ•°"] / df_feat["äºˆç´„ä»¶æ•°"]
    return df_feat.fillna(0)


def generate_weight_features(past_raw, target_items, holidays):
    df_pivot = (
        past_raw.groupby(["ä¼ç¥¨æ—¥ä»˜", "å“å"])["æ­£å‘³é‡é‡"].sum().unstack(fill_value=0)
    )
    for item in target_items:
        if item not in df_pivot.columns:
            df_pivot[item] = 0
    df_pivot = df_pivot.sort_index()
    df_pivot["åˆè¨ˆ"] = df_pivot[target_items].sum(axis=1)

    df_feat = pd.DataFrame(index=df_pivot.index)
    for item in target_items:
        df_feat[f"{item}_å‰æ—¥å€¤"] = df_pivot[item].shift(1)
        df_feat[f"{item}_å‰é€±å¹³å‡"] = df_pivot[item].shift(1).rolling(7).mean()
    df_feat["åˆè¨ˆ_å‰æ—¥å€¤"] = df_pivot["åˆè¨ˆ"].shift(1)
    df_feat["åˆè¨ˆ_3æ—¥å¹³å‡"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).mean()
    df_feat["åˆè¨ˆ_3æ—¥åˆè¨ˆ"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).sum()
    df_feat["åˆè¨ˆ_å‰é€±å¹³å‡"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(7).mean()

    daily_avg = past_raw.groupby("ä¼ç¥¨æ—¥ä»˜")["æ­£å‘³é‡é‡"].median()
    df_feat["1å°ã‚ãŸã‚Šé‡é‡_éå»ä¸­å¤®å€¤"] = (
        daily_avg.shift(1).rolling(60, min_periods=10).median()
    )

    df_feat["æ›œæ—¥"] = df_feat.index.dayofweek
    df_feat["é€±ç•ªå·"] = df_feat.index.isocalendar().week
    holiday_dates = pd.to_datetime(holidays)
    df_feat["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df_feat.index.isin(holiday_dates).astype(int)
    df_feat["ç¥æ—¥å‰ãƒ•ãƒ©ã‚°"] = df_feat.index.map(
        lambda d: (d + pd.Timedelta(days=1)) in holiday_dates
    ).astype(int)
    df_feat["ç¥æ—¥å¾Œãƒ•ãƒ©ã‚°"] = df_feat.index.map(
        lambda d: (d - pd.Timedelta(days=1)) in holiday_dates
    ).astype(int)

    df_feat = df_feat.dropna()
    df_pivot = df_pivot.loc[df_feat.index]
    return df_feat, df_pivot


def train_and_predict_stage1(
    df_feat_today,
    df_past_feat,
    df_past_pivot,
    base_models,
    meta_model_proto,
    feature_list,
    target_items,
    stage1_eval,
    df_pivot,
):
    print("â–¶ï¸ train_and_predict_stage1 é–‹å§‹")
    print("ğŸ“Œ df_feat_today index:", df_feat_today.index)
    print("ğŸ“Œ å­¦ç¿’ç”¨ç‰¹å¾´é‡ã‚µã‚¤ã‚º:", df_past_feat.shape)
    print("ğŸ“Œ å­¦ç¿’ç”¨pivotã‚µã‚¤ã‚º:", df_past_pivot.shape)

    results = {}
    X_train = df_past_feat[feature_list]
    for item in target_items:
        y_train = df_past_pivot[item]
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        selector = VarianceThreshold(1e-4)
        X_train_filtered = selector.fit_transform(X_train_scaled)

        trained_models = []
        for name, model in base_models:
            print(f"ğŸ›  ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­: {name} for {item}")
            m = clone(model)
            m.fit(X_train_filtered, y_train)
            trained_models.append(m)

        train_meta = np.column_stack(
            [m.predict(X_train_filtered) for m in trained_models]
        )
        meta_model = clone(meta_model_proto)
        meta_model.fit(train_meta, y_train)

        X_target = df_feat_today[feature_list]
        X_target_scaled = scaler.transform(X_target)
        X_target_filtered = selector.transform(X_target_scaled)

        meta_input = np.column_stack(
            [m.predict(X_target_filtered) for m in trained_models]
        )
        pred = meta_model.predict(meta_input)[0]
        results[f"{item}_äºˆæ¸¬"] = pred

        true_val = df_pivot.loc[df_feat_today.index[0], item]
        stage1_eval[item]["y_true"].append(true_val)
        stage1_eval[item]["y_pred"].append(pred)

        print(f"âœ… {item} äºˆæ¸¬: {pred:.1f}kg / æ­£è§£: {true_val:.1f}kg")

        # ç‰¹å¾´é‡é‡è¦åº¦ã®å‡ºåŠ›
        selected_columns = np.array(feature_list)[selector.get_support()]

        # ElasticNetã®ä¿‚æ•°ï¼ˆçµ¶å¯¾å€¤é †ï¼‰
        if hasattr(trained_models[0], "coef_"):
            elastic_coef = trained_models[0].coef_
            print(f"ğŸ” ElasticNet ä¿‚æ•° ({item}, çµ¶å¯¾å€¤é †):")
            for name, val in sorted(
                zip(selected_columns, elastic_coef),
                key=lambda x: -abs(x[1]),  # â†çµ¶å¯¾å€¤ã§é™é †ã‚½ãƒ¼ãƒˆ
            ):
                print(f"   {name:<25} : {val:.4f}")

        # RandomForestã®ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆé«˜ã„é †ï¼‰
        if hasattr(trained_models[1], "feature_importances_"):
            rf_importances = trained_models[1].feature_importances_
            print(f"ğŸ” RandomForest é‡è¦åº¦ ({item}, ä¸Šä½10ä»¶):")
            for name, val in sorted(
                zip(selected_columns, rf_importances), key=lambda x: -x[1]
            )[:10]:
                print(f"   {name:<25} : {val:.4f}")

    return results


def train_and_predict_stage2(
    all_stage1_rows, stage1_results, df_feat_today, target_items
):
    print("â–¶ï¸ train_and_predict_stage2 é–‹å§‹")
    df_hist = pd.DataFrame(all_stage1_rows[:-1])
    print("ğŸ“Œ df_hist ã‚µã‚¤ã‚º:", df_hist.shape)

    X_train = df_hist.drop(columns=["åˆè¨ˆ"])
    y_train = df_hist["åˆè¨ˆ"]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    selector = VarianceThreshold(1e-4)
    X_train_filtered = selector.fit_transform(X_train_scaled)

    gbdt = GradientBoostingRegressor(
        n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42
    )
    gbdt.fit(X_train_filtered, y_train)

    X_target = {
        f"{item}_äºˆæ¸¬": [stage1_results[f"{item}_äºˆæ¸¬"]] for item in target_items
    }
    for col in df_feat_today.columns:
        if col not in X_target:
            X_target[col] = df_feat_today.iloc[0][col]
    X_target = pd.DataFrame(X_target)

    X_target_scaled = scaler.transform(X_target)
    X_target_filtered = selector.transform(X_target_scaled)

    total_pred = gbdt.predict(X_target_filtered)[0]
    print(f"âœ… åˆè¨ˆäºˆæ¸¬: {total_pred:.1f}kg")

    # GBDTç‰¹å¾´é‡é‡è¦åº¦
    X_cols = X_train.columns[selector.get_support()]
    gbdt_importance = gbdt.feature_importances_
    print("ğŸ” GBDT ç‰¹å¾´é‡é‡è¦åº¦ (ä¸Šä½10):")
    for name, val in sorted(zip(X_cols, gbdt_importance), key=lambda x: -x[1])[:10]:
        print(f"   {name:<25} : {val:.4f}")

    return total_pred


from sklearn.metrics import mean_squared_error


def evaluate_stage1(stage1_eval, target_items):
    print("\n===== ã‚¹ãƒ†ãƒ¼ã‚¸1è©•ä¾¡çµæœ =====")
    for item in target_items:
        y_true = np.array(stage1_eval[item]["y_true"])
        y_pred = np.array(stage1_eval[item]["y_pred"])
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        max_error = np.max(np.abs(y_true - y_pred))
        print(
            f"{item}: RÂ² = {r2:.3f}, MAE = {mae:,.0f}kg, RMSE = {rmse:,.0f}kg, æœ€å¤§èª¤å·® = {max_error:,.0f}kg"
        )


def full_walkforward(
    df_raw, holidays, df_reserve, min_stage1_days, min_stage2_days, top_n=5
):
    print("â–¶ï¸ full_walkforward é–‹å§‹")
    print("ğŸ“Œ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ä»¶æ•°:", len(df_raw))

    df_raw["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_raw["ä¼ç¥¨æ—¥ä»˜"])
    df_raw = df_raw.sort_values("ä¼ç¥¨æ—¥ä»˜")
    target_items = get_target_items(df_raw, top_n)
    df_feat, df_pivot = generate_weight_features(df_raw, target_items, holidays)

    base_models = [
        ("elastic", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000, tol=1e-2)),
        ("rf", RandomForestRegressor(n_estimators=100, random_state=42)),
    ]
    meta_model_proto = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000, tol=1e-2)

    feature_list = [
        *[f"{item}_å‰æ—¥å€¤" for item in target_items],
        *[f"{item}_å‰é€±å¹³å‡" for item in target_items],
        "åˆè¨ˆ_å‰æ—¥å€¤",
        "åˆè¨ˆ_3æ—¥å¹³å‡",
        # "åˆè¨ˆ_3æ—¥åˆè¨ˆ",
        "åˆè¨ˆ_å‰é€±å¹³å‡",
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "1å°ã‚ãŸã‚Šé‡é‡_éå»ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
        "ç¥æ—¥å‰ãƒ•ãƒ©ã‚°",
        "ç¥æ—¥å¾Œãƒ•ãƒ©ã‚°",
        "äºˆç´„ä»¶æ•°",
        "äºˆç´„åˆè¨ˆå°æ•°",
        "å›ºå®šå®¢äºˆç´„æ•°",
        # "éå›ºå®šå®¢äºˆç´„æ•°",
        # "å›ºå®šå®¢æ¯”ç‡",
        "ä¸Šä½å¾—æ„å…ˆäºˆç´„æ•°",
    ]

    all_actual, all_pred, all_stage1_rows = [], [], []
    stage1_eval = {item: {"y_true": [], "y_pred": []} for item in target_items}
    dates = df_feat.index

    for i, target_date in enumerate(dates):
        if i < min_stage1_days:
            continue

        df_past_feat = df_feat[df_feat.index < target_date].tail(600)
        df_past_pivot = df_pivot.loc[df_past_feat.index]

        df_reserve_filtered = df_reserve[
            pd.to_datetime(df_reserve["äºˆç´„æ—¥"]) <= target_date
        ]
        df_reserve_feat_all = generate_reserve_features(df_reserve_filtered)

        df_past_feat = df_past_feat.merge(
            df_reserve_feat_all, left_index=True, right_index=True, how="left"
        ).fillna(0)

        df_feat_today = df_feat.loc[[target_date]].copy()
        df_feat_today = df_feat_today.merge(
            df_reserve_feat_all, left_index=True, right_index=True, how="left"
        ).fillna(0)

        print(f"\n=== {target_date.strftime('%Y-%m-%d')} ã‚’äºˆæ¸¬ä¸­ ===")
        stage1_result = train_and_predict_stage1(
            df_feat_today,
            df_past_feat,
            df_past_pivot,
            base_models,
            meta_model_proto,
            feature_list,
            target_items,
            stage1_eval,
            df_pivot,  # âœ… è¿½åŠ 
        )

        row = {f"{item}_äºˆæ¸¬": stage1_result[f"{item}_äºˆæ¸¬"] for item in target_items}
        for col in df_feat_today.columns:
            if col not in row:
                row[col] = df_feat_today.iloc[0][col]
        row["åˆè¨ˆ"] = df_pivot.loc[target_date, "åˆè¨ˆ"]
        all_stage1_rows.append(row)

        if len(all_stage1_rows) > min_stage2_days:
            total_pred = train_and_predict_stage2(
                all_stage1_rows, stage1_result, df_feat_today, target_items
            )
            total_actual = df_pivot.loc[target_date, "åˆè¨ˆ"]
            all_actual.append(total_actual)
            all_pred.append(total_pred)

    print("\n===== ã‚¹ãƒ†ãƒ¼ã‚¸2è©•ä¾¡çµæœ (åˆè¨ˆ) =====")
    if all_actual:
        r2 = r2_score(all_actual, all_pred)
        mae = mean_absolute_error(all_actual, all_pred)
        rmse = np.sqrt(mean_squared_error(all_actual, all_pred))
        max_error = np.max(np.abs(np.array(all_actual) - np.array(all_pred)))
        print(
            f"RÂ² = {r2:.3f}, MAE = {mae:,.0f}kg, RMSE = {rmse:,.0f}kg, æœ€å¤§èª¤å·® = {max_error:,.0f}kg"
        )
    else:
        print("è©•ä¾¡ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

    evaluate_stage1(stage1_eval, target_items)
    return all_actual, all_pred


import matplotlib.pyplot as plt


# é‡è¦åº¦å¯è¦–åŒ–
def plot_feature_importances(names, importances, title="Feature Importance", top_k=15):
    sorted_idx = np.argsort(importances)[-top_k:]
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_idx)), np.array(importances)[sorted_idx], align="center")
    plt.yticks(range(len(sorted_idx)), np.array(names)[sorted_idx])
    plt.title(title)
    plt.xlabel("Importance")
    plt.tight_layout()
    plt.show()
