import pandas as pd
import numpy as np
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.base import clone
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
import matplotlib.pyplot as plt

# === ç‰¹å¾´é‡ä½œæˆï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã”ã¨ã«åˆ†å‰²ï¼‰ ===
from logic.factory_manage.new_model1.feature_builder import (
    WeightFeatureBuilder,
    ReserveFeatureBuilder,
)


def get_target_items(df_raw, top_n=5):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ä¸Šä½ã®å“ç›®ã‚’å–å¾—ã—ã¾ã™ã€‚

    Args:
        df_raw (pd.DataFrame): å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        top_n (int): å–å¾—ã™ã‚‹å“ç›®ã®æ•°ã€‚

    Returns:
        list: ä¸Šä½ã®å“ç›®ãƒªã‚¹ãƒˆã€‚
    """
    return df_raw["å“å"].value_counts().head(top_n).index.tolist()


def train_and_predict_stage1(
    df_feat_today,
    df_past_feat,
    df_past_pivot,
    base_models,
    meta_model_proto,
    feature_list,
    target_items,
    stage1_eval,
    df_pivot,
):
    """
    ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚

    Args:
        df_feat_today (pd.DataFrame): ä»Šæ—¥ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã€‚
        df_past_feat (pd.DataFrame): éå»ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã€‚
        df_past_pivot (pd.DataFrame): éå»ã®ãƒ”ãƒœãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã€‚
        base_models (list): åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã€‚
        meta_model_proto (object): ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€‚
        feature_list (list): ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆã€‚
        target_items (list): äºˆæ¸¬å¯¾è±¡ã®å“ç›®ãƒªã‚¹ãƒˆã€‚
        stage1_eval (dict): ã‚¹ãƒ†ãƒ¼ã‚¸1ã®è©•ä¾¡çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸ã€‚
        df_pivot (pd.DataFrame): ãƒ”ãƒœãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã€‚

    Returns:
        dict: å“ç›®ã”ã¨ã®äºˆæ¸¬çµæœã€‚
    """
    print("\nâ–¶ï¸ train_and_predict_stage1 é–‹å§‹")

    results = {}
    X_train = df_past_feat[feature_list]
    for item in target_items:
        y_train = df_past_pivot[item]
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        selector = VarianceThreshold(1e-4)
        X_train_filtered = selector.fit_transform(X_train_scaled)

        trained_models = []
        for name, model in base_models:
            print(f"ğŸ›  ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­: {name} for {item}")
            m = clone(model)
            m.fit(X_train_filtered, y_train)
            trained_models.append(m)

        meta_input_train = np.column_stack(
            [m.predict(X_train_filtered) for m in trained_models]
        )
        meta_model = clone(meta_model_proto)
        meta_model.fit(meta_input_train, y_train)

        X_target = df_feat_today[feature_list]
        X_target_scaled = scaler.transform(X_target)
        X_target_filtered = selector.transform(X_target_scaled)
        meta_input_target = np.column_stack(
            [m.predict(X_target_filtered) for m in trained_models]
        )
        pred = meta_model.predict(meta_input_target)[0]
        results[f"{item}_äºˆæ¸¬"] = pred

        true_val = df_pivot.loc[df_feat_today.index[0], item]
        stage1_eval[item]["y_true"].append(true_val)
        stage1_eval[item]["y_pred"].append(pred)

    return results


def train_and_predict_stage2(
    all_stage1_rows, stage1_results, df_feat_today, target_items
):
    """
    ã‚¹ãƒ†ãƒ¼ã‚¸2ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚

    Args:
        all_stage1_rows (list): ã‚¹ãƒ†ãƒ¼ã‚¸1ã®å…¨è¡Œãƒ‡ãƒ¼ã‚¿ã€‚
        stage1_results (dict): ã‚¹ãƒ†ãƒ¼ã‚¸1ã®äºˆæ¸¬çµæœã€‚
        df_feat_today (pd.DataFrame): ä»Šæ—¥ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã€‚
        target_items (list): äºˆæ¸¬å¯¾è±¡ã®å“ç›®ãƒªã‚¹ãƒˆã€‚

    Returns:
        tuple: åˆè¨ˆäºˆæ¸¬å€¤ã€ç‰¹å¾´é‡åã€é‡è¦åº¦ã€‚
    """
    print("\nâ–¶ï¸ train_and_predict_stage2 é–‹å§‹")
    df_hist = pd.DataFrame(all_stage1_rows[:-1])

    X_train = df_hist.drop(columns=["åˆè¨ˆ"])
    y_train = df_hist["åˆè¨ˆ"]

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    selector = VarianceThreshold(1e-4)
    X_train_filtered = selector.fit_transform(X_train_scaled)

    gbdt = GradientBoostingRegressor(
        n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42
    )
    gbdt.fit(X_train_filtered, y_train)

    # â†â˜… ç‰¹å¾´é‡åã®æŠ½å‡ºï¼ˆselector ã‚’é€šéã—ãŸã‚‚ã®ã ã‘ï¼‰
    feature_names = np.array(X_train.columns)[selector.get_support()]
    importances = gbdt.feature_importances_

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    X_target = {
        f"{item}_äºˆæ¸¬": [stage1_results[f"{item}_äºˆæ¸¬"]] for item in target_items
    }
    for col in df_feat_today.columns:
        if col not in X_target:
            X_target[col] = df_feat_today.iloc[0][col]
    X_target = pd.DataFrame(X_target)

    X_target_scaled = scaler.transform(X_target)
    X_target_filtered = selector.transform(X_target_scaled)
    total_pred = gbdt.predict(X_target_filtered)[0]

    print(f"âœ… åˆè¨ˆäºˆæ¸¬: {total_pred:.1f}kg")

    return total_pred, feature_names, importances


def evaluate_stage1(stage1_eval, target_items):
    """
    ã‚¹ãƒ†ãƒ¼ã‚¸1ã®è©•ä¾¡çµæœã‚’è¨ˆç®—ã—ã€è¡¨ç¤ºã—ã¾ã™ã€‚

    Args:
        stage1_eval (dict): ã‚¹ãƒ†ãƒ¼ã‚¸1ã®è©•ä¾¡çµæœã€‚
        target_items (list): äºˆæ¸¬å¯¾è±¡ã®å“ç›®ãƒªã‚¹ãƒˆã€‚

    Returns:
        None
    """
    print("\n===== ã‚¹ãƒ†ãƒ¼ã‚¸1è©•ä¾¡çµæœ =====")
    for item in target_items:
        y_true = np.array(stage1_eval[item]["y_true"])
        y_pred = np.array(stage1_eval[item]["y_pred"])
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        max_error = np.max(np.abs(y_true - y_pred))
        print(
            f"{item}: RÂ² = {r2:.3f}, MAE = {mae:,.0f}kg, RMSE = {rmse:,.0f}kg, æœ€å¤§èª¤å·® = {max_error:,.0f}kg"
        )


def evaluate_stage1_feature_importance_as_table(records, top_k=15):
    """
    ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤ºã—ã¾ã™ã€‚

    Args:
        records (list): ç‰¹å¾´é‡é‡è¦åº¦ã®è¨˜éŒ²ã€‚
        top_k (int): è¡¨ç¤ºã™ã‚‹ä¸Šä½ã®ä»¶æ•°ã€‚

    Returns:
        pd.DataFrame: ç‰¹å¾´é‡é‡è¦åº¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """
    df = pd.DataFrame(records)

    # ãƒ¢ãƒ‡ãƒ«ãƒ»å“ç›®ãƒ»ç‰¹å¾´é‡ã”ã¨ã«å¹³å‡ã‚’å–ã‚‹ï¼ˆè¤‡æ•°æ—¥åˆ†ã‚ã‚Œã°ï¼‰
    df_summary = (
        df.groupby(["å“ç›®", "ãƒ¢ãƒ‡ãƒ«", "ç‰¹å¾´é‡"])["é‡è¦åº¦"]
        .mean()
        .reset_index()
        .sort_values("é‡è¦åº¦", ascending=False)
    )

    print("\n===== ã‚¹ãƒ†ãƒ¼ã‚¸1 ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° =====")
    print(df_summary.head(top_k).to_string(index=False))

    return df_summary


def full_walkforward(
    df_raw, holidays, df_reserve, min_stage1_days, min_stage2_days, top_n=5
):
    """
    ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰æ³•ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    Args:
        df_raw (pd.DataFrame): å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        holidays (list): ç¥æ—¥ã®ãƒªã‚¹ãƒˆã€‚
        df_reserve (pd.DataFrame): äºˆç´„ãƒ‡ãƒ¼ã‚¿ã€‚
        df_raw (pd.DataFrame): å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
        holidays (list): ç¥æ—¥ã®ãƒªã‚¹ãƒˆã€‚
        df_reserve (pd.DataFrame): äºˆç´„ãƒ‡ãƒ¼ã‚¿ã€‚
        min_stage1_days (int): ã‚¹ãƒ†ãƒ¼ã‚¸1ã®æœ€å°æ—¥æ•°ã€‚
        min_stage2_days (int): ã‚¹ãƒ†ãƒ¼ã‚¸2ã®æœ€å°æ—¥æ•°ã€‚
        top_n (int): ä¸Šä½ã®å“ç›®æ•°ã€‚

    Returns:
        tuple: å®Ÿéš›ã®å€¤ã¨äºˆæ¸¬å€¤ã®ãƒªã‚¹ãƒˆã€‚
    """
    print("â–¶ï¸ full_walkforward é–‹å§‹")

    df_raw["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_raw["ä¼ç¥¨æ—¥ä»˜"])
    df_raw = df_raw.sort_values("ä¼ç¥¨æ—¥ä»˜")
    target_items = get_target_items(df_raw, top_n)

    builder = WeightFeatureBuilder(df_raw, target_items, holidays)
    df_feat, df_pivot = builder.build()

    reserve_builder = ReserveFeatureBuilder(df_reserve)
    df_reserve_feat_all = reserve_builder.build()

    base_models = [
        ("elastic", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000, tol=1e-2)),
        ("rf", RandomForestRegressor(n_estimators=100, random_state=42)),
    ]
    meta_model_proto = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000, tol=1e-2)

    feature_list = [
        *[f"{item}_å‰æ—¥å€¤" for item in target_items],
        *[f"{item}_å‰é€±å¹³å‡" for item in target_items],
        "åˆè¨ˆ_å‰æ—¥å€¤",
        "åˆè¨ˆ_3æ—¥å¹³å‡",
        "åˆè¨ˆ_å‰é€±å¹³å‡",
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "1å°ã‚ãŸã‚Šé‡é‡_éå»ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
        "ç¥æ—¥å‰ãƒ•ãƒ©ã‚°",
        "ç¥æ—¥å¾Œãƒ•ãƒ©ã‚°",
        "ç¬¬2æ—¥æ›œãƒ•ãƒ©ã‚°",
        "é€£ä¼‘å‰ãƒ•ãƒ©ã‚°",
        "é€£ä¼‘å¾Œãƒ•ãƒ©ã‚°",
        "äºˆç´„ä»¶æ•°",
        "äºˆç´„åˆè¨ˆå°æ•°",
        "å›ºå®šå®¢äºˆç´„æ•°",
        "ä¸Šä½å¾—æ„å…ˆäºˆç´„æ•°",
    ]

    all_actual, all_pred, all_stage1_rows = [], [], []
    stage1_feat_importances = []
    stage1_eval = {item: {"y_true": [], "y_pred": []} for item in target_items}
    dates = df_feat.index

    for i, target_date in enumerate(dates):
        if i < min_stage1_days:
            continue

        df_past_feat = df_feat[df_feat.index < target_date].tail(600)
        df_past_pivot = df_pivot.loc[df_past_feat.index]

        df_reserve_today = df_reserve_feat_all[df_reserve_feat_all.index <= target_date]
        df_past_feat = df_past_feat.merge(
            df_reserve_today, left_index=True, right_index=True, how="left"
        ).fillna(0)
        df_feat_today = df_feat.loc[[target_date]].copy()
        df_feat_today = df_feat_today.merge(
            df_reserve_today, left_index=True, right_index=True, how="left"
        ).fillna(0)

        print(f"\n=== {target_date.strftime('%Y-%m-%d')} ã‚’äºˆæ¸¬ä¸­ ===")
        stage1_result = train_and_predict_stage1(
            df_feat_today,
            df_past_feat,
            df_past_pivot,
            base_models,
            meta_model_proto,
            feature_list,
            target_items,
            stage1_eval,
            df_pivot,
        )

        row = {f"{item}_äºˆæ¸¬": stage1_result[f"{item}_äºˆæ¸¬"] for item in target_items}
        for col in df_feat_today.columns:
            if col not in row:
                row[col] = df_feat_today.iloc[0][col]
        row["åˆè¨ˆ"] = df_pivot.loc[target_date, "åˆè¨ˆ"]
        all_stage1_rows.append(row)

        if len(all_stage1_rows) > min_stage2_days:
            total_pred = train_and_predict_stage2(
                all_stage1_rows, stage1_result, df_feat_today, target_items
            )
            total_actual = df_pivot.loc[target_date, "åˆè¨ˆ"]
            all_actual.append(total_actual)
            all_pred.append(total_pred)

    print("\n===== ã‚¹ãƒ†ãƒ¼ã‚¸2è©•ä¾¡çµæœ (åˆè¨ˆ) =====")
    if all_actual:
        r2 = r2_score(all_actual, all_pred)
        mae = mean_absolute_error(all_actual, all_pred)
        rmse = np.sqrt(mean_squared_error(all_actual, all_pred))
        max_error = np.max(np.abs(np.array(all_actual) - np.array(all_pred)))
        print(
            f"RÂ² = {r2:.3f}, MAE = {mae:,.0f}kg, RMSE = {rmse:,.0f}kg, æœ€å¤§èª¤å·® = {max_error:,.0f}kg"
        )
    else:
        print("è©•ä¾¡ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

    evaluate_stage1(stage1_eval, target_items)
    # ã‚¹ãƒ†ãƒ¼ã‚¸2è©•ä¾¡ã®å¾Œã«
    evaluate_stage1_feature_importance_as_table(stage1_feat_importances, top_k=20)
    return all_actual, all_pred


def plot_feature_importances(names, importances, title="Feature Importance", top_k=15):
    """
    ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚

    Args:
        names (list): ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆã€‚
        importances (list): ç‰¹å¾´é‡ã®é‡è¦åº¦ã€‚
        title (str): ãƒ—ãƒ­ãƒƒãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ã€‚
        top_k (int): è¡¨ç¤ºã™ã‚‹ä¸Šä½ã®ä»¶æ•°ã€‚

    Returns:
        None
    """
    sorted_idx = np.argsort(importances)[-top_k:]
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_idx)), np.array(importances)[sorted_idx], align="center")
    plt.yticks(range(len(sorted_idx)), np.array(names)[sorted_idx])
    plt.title(title)
    plt.xlabel("Importance")
    plt.tight_layout()
    plt.show()


def evaluate_feature_importance_as_table(feature_names, importances, top_k=15):
    """
    ç‰¹å¾´é‡é‡è¦åº¦ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤ºã—ã¾ã™ã€‚

    Args:
        feature_names (list): ç‰¹å¾´é‡åã®ãƒªã‚¹ãƒˆã€‚
        importances (list): ç‰¹å¾´é‡ã®é‡è¦åº¦ã€‚
        top_k (int): è¡¨ç¤ºã™ã‚‹ä¸Šä½ã®ä»¶æ•°ã€‚

    Returns:
        pd.DataFrame: ç‰¹å¾´é‡é‡è¦åº¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚
    """
    df_importance = pd.DataFrame(
        {"ç‰¹å¾´é‡": feature_names, "é‡è¦åº¦": importances}
    ).sort_values("é‡è¦åº¦", ascending=False)

    top_df = df_importance.head(top_k)
    print("\n===== ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° =====")
    print(top_df.to_string(index=False))
    return top_df
