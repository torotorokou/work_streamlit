import pandas as pd
import numpy as np
import joblib

# === ãƒ¢ãƒ‡ãƒ« ===
from sklearn.linear_model import Ridge, ElasticNet, LogisticRegression
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    GradientBoostingClassifier,
)

# === å­¦ç¿’ç³» ===
from sklearn.model_selection import train_test_split, KFold
from sklearn.base import clone

# === è©•ä¾¡æŒ‡æ¨™ ===
from sklearn.metrics import (
    r2_score,
    mean_absolute_error,
    accuracy_score,
    roc_auc_score,
)

# === ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
from utils.config_loader import get_path_from_yaml
from utils.get_holydays import get_japanese_holidays
from logic.factory_manage.sql import load_data_from_sqlite

# ç¥æ—¥ãƒ•ãƒ©ã‚°ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»äºˆæ¸¬ã‚’ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹é–¢æ•°


def maesyori():
    base_dir = get_path_from_yaml("input", section="directories")
    df_raw = pd.read_csv(f"{base_dir}/20240501-20250422.csv", encoding="utf-8")
    df_raw = df_raw[["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡", "å“å"]]

    df2 = pd.read_csv(f"{base_dir}/2020é¡§å®¢.csv")[["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]]
    df3 = pd.read_csv(f"{base_dir}/2021é¡§å®¢.csv")[["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]]
    df4 = pd.read_csv(f"{base_dir}/2023_all.csv")[["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]]

    df_all = pd.concat([df2, df3, df4])
    df_all.rename(columns={"å•†å“": "å“å", "æ­£å‘³é‡é‡_æ˜ç´°": "æ­£å‘³é‡é‡"}, inplace=True)
    df_all["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_all["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")

    df_raw = pd.concat([df_raw, df_all])

    # ğŸ”§ ä¿®æ­£ï¼šstr.replaceã®å‰ã«strå‹ã¸æ˜ç¤ºçš„ã«å¤‰æ›
    df_raw["ä¼ç¥¨æ—¥ä»˜"] = (
        df_raw["ä¼ç¥¨æ—¥ä»˜"].astype(str).str.replace(r"\(.*\)", "", regex=True)
    )
    df_raw["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_raw["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")
    df_raw["æ­£å‘³é‡é‡"] = pd.to_numeric(df_raw["æ­£å‘³é‡é‡"], errors="coerce")

    # df_raw = df_raw.dropna(subset=["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"])  # ğŸ”’ æ—¥ä»˜ã‚‚NaTé™¤å»

    return df_raw


def train_model_with_holiday(df_raw: pd.DataFrame, holidays: list[str]) -> dict:
    # --- ç‰¹å¾´é‡ç”Ÿæˆ ---
    df_pivot = (
        df_raw.groupby(["ä¼ç¥¨æ—¥ä»˜", "å“å"])["æ­£å‘³é‡é‡"].sum().unstack(fill_value=0)
    )
    df_pivot["åˆè¨ˆ"] = df_pivot.sum(axis=1)

    df_feat = pd.DataFrame(index=df_pivot.index)
    df_feat["æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥"] = df_pivot["æ··åˆå»ƒæ£„ç‰©A"].shift(1)
    df_feat["æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥"] = df_pivot["æ··åˆå»ƒæ£„ç‰©B"].shift(1)
    df_feat["åˆè¨ˆ_å‰æ—¥"] = df_pivot["åˆè¨ˆ"].shift(1)
    df_feat["åˆè¨ˆ_3æ—¥å¹³å‡"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).mean()
    df_feat["åˆè¨ˆ_3æ—¥åˆè¨ˆ"] = df_pivot["åˆè¨ˆ"].shift(1).rolling(3).sum()
    df_feat["æ›œæ—¥"] = df_feat.index.dayofweek
    df_feat["é€±ç•ªå·"] = df_feat.index.isocalendar().week

    daily_avg = df_raw.groupby("ä¼ç¥¨æ—¥ä»˜")["æ­£å‘³é‡é‡"].median()
    df_feat["1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤"] = daily_avg.shift(1).expanding().median()

    holiday_dates = pd.to_datetime(holidays)
    df_feat["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df_feat.index.isin(holiday_dates).astype(int)

    df_feat = df_feat.dropna()
    df_pivot = df_pivot.loc[df_feat.index]

    # --- å­¦ç¿’ç”¨è¨­å®š ---
    ab_features = [
        "æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥",
        "æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥",
        "åˆè¨ˆ_å‰æ—¥",
        "åˆè¨ˆ_3æ—¥å¹³å‡",
        "åˆè¨ˆ_3æ—¥åˆè¨ˆ",
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
    ]
    target_items = ["æ··åˆå»ƒæ£„ç‰©A", "æ··åˆå»ƒæ£„ç‰©B", "æ··åˆå»ƒæ£„ç‰©(ï½¿ï¾Œï½§ï½°ï½¥å®¶å…·é¡)"]

    base_models = [
        ("elastic", ElasticNet(alpha=0.1, l1_ratio=0.5)),
        ("rf", RandomForestRegressor(n_estimators=100, random_state=42)),
    ]
    meta_model_stage1 = ElasticNet(alpha=0.1, l1_ratio=0.5)
    gbdt_model = GradientBoostingRegressor(
        n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42
    )
    clf_model = GradientBoostingClassifier(
        n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42
    )
    kf = KFold(n_splits=5)

    X_features_all = {}
    stacked_preds = {}

    for item in target_items:
        X = (
            df_feat[ab_features]
            if item == "æ··åˆå»ƒæ£„ç‰©A"
            else df_feat[[c for c in ab_features if "1å°ã‚ãŸã‚Š" not in c]]
        )
        y = df_pivot[item]
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        X_features_all[item] = X
        train_meta = np.zeros((X_train.shape[0], len(base_models)))

        for i, (_, model) in enumerate(base_models):
            for train_idx, val_idx in kf.split(X_train):
                model_ = clone(model)
                model_.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])
                train_meta[val_idx, i] = model_.predict(X_train.iloc[val_idx])

        meta_model_stage1.fit(train_meta, y_train)

        test_meta = np.column_stack(
            [
                clone(model).fit(X_train, y_train).predict(X_test)
                for _, model in base_models
            ]
        )
        stacked_preds[item] = meta_model_stage1.predict(test_meta)

        # --- æ±åŒ–æ€§èƒ½ã®è¡¨ç¤º ---
        y_train_pred = meta_model_stage1.predict(train_meta)
        y_test_pred = stacked_preds[item]

        print(f"\nğŸ“˜ {item}")
        print(f"ã€€ã€€RÂ² (train) = {r2_score(y_train, y_train_pred):.3f}")
        print(f"ã€€ã€€RÂ² (test)  = {r2_score(y_test, y_test_pred):.3f}")
        print(f"ã€€ã€€MAE (train) = {mean_absolute_error(y_train, y_train_pred):,.0f} kg")
        print(f"ã€€ã€€MAE (test)  = {mean_absolute_error(y_test, y_test_pred):,.0f} kg")

    index_final = X_test.index
    df_stage1 = pd.DataFrame(
        {f"{k}_äºˆæ¸¬": v for k, v in stacked_preds.items()}, index=index_final
    )
    for col in [
        "æ›œæ—¥",
        "é€±ç•ªå·",
        "åˆè¨ˆ_å‰æ—¥",
        "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
        "ç¥æ—¥ãƒ•ãƒ©ã‚°",
    ]:
        df_stage1[col] = df_feat.loc[index_final, col]

    # --- åˆè¨ˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡ ---
    y_total_final = df_pivot.loc[df_stage1.index, "åˆè¨ˆ"]
    gbdt_model.fit(df_stage1, y_total_final)
    y_pred_total = gbdt_model.predict(df_stage1)

    print("\nğŸ“˜ åˆè¨ˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆGBDTï¼‰")
    print(f"ã€€ã€€RÂ² = {r2_score(y_total_final, y_pred_total):.3f}")
    print(f"ã€€ã€€MAE = {mean_absolute_error(y_total_final, y_pred_total):,.0f} kg")

    # --- åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ---
    y_total_binary = (y_total_final < 90000).astype(int)
    clf_model.fit(df_stage1.drop(columns=["ç¥æ—¥ãƒ•ãƒ©ã‚°"]), y_total_binary)

    residuals = y_total_final - y_pred_total

    return {
        "meta_model": meta_model_stage1,
        "base_models": base_models,
        "gbdt_model": gbdt_model,
        "clf_model": clf_model,
        "X_features_all": X_features_all,
        "df_feat": df_feat,
        "df_pivot": df_pivot,
        "ab_features": ab_features,
        "target_items": target_items,
        "holiday_dates": holiday_dates,
        "bias": residuals.mean(),
        "std": residuals.std(),
    }


def predict_with_model(
    model_dict: dict, start_date: str, end_date: str
) -> pd.DataFrame:
    results = []
    predict_dates = pd.date_range(start=start_date, end=end_date)
    last_date = model_dict["df_feat"].index[-1]

    for predict_date in predict_dates:
        new_row = {
            "æ··åˆå»ƒæ£„ç‰©A_å‰æ—¥": model_dict["df_pivot"].loc[last_date, "æ··åˆå»ƒæ£„ç‰©A"],
            "æ··åˆå»ƒæ£„ç‰©B_å‰æ—¥": model_dict["df_pivot"].loc[last_date, "æ··åˆå»ƒæ£„ç‰©B"],
            "åˆè¨ˆ_å‰æ—¥": model_dict["df_pivot"].loc[last_date, "åˆè¨ˆ"],
            "åˆè¨ˆ_3æ—¥å¹³å‡": model_dict["df_pivot"]["åˆè¨ˆ"]
            .shift(1)
            .rolling(3)
            .mean()
            .loc[last_date],
            "åˆè¨ˆ_3æ—¥åˆè¨ˆ": model_dict["df_pivot"]["åˆè¨ˆ"]
            .shift(1)
            .rolling(3)
            .sum()
            .loc[last_date],
            "æ›œæ—¥": predict_date.dayofweek,
            "é€±ç•ªå·": predict_date.isocalendar().week,
            "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤": model_dict["df_feat"][
                "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤"
            ].iloc[-1],
            "ç¥æ—¥ãƒ•ãƒ©ã‚°": int(predict_date in model_dict["holiday_dates"]),
        }
        df_input = pd.DataFrame(new_row, index=[predict_date])
        for item in model_dict["target_items"]:
            x_item = (
                df_input[model_dict["ab_features"]]
                if item == "æ··åˆå»ƒæ£„ç‰©A"
                else df_input[
                    [c for c in model_dict["ab_features"] if "1å°ã‚ãŸã‚Š" not in c]
                ]
            )
            meta_input = np.column_stack(
                [
                    clone(model)
                    .fit(
                        model_dict["X_features_all"][item],
                        model_dict["df_pivot"].loc[model_dict["df_feat"].index, item],
                    )
                    .predict(x_item)
                    for _, model in model_dict["base_models"]
                ]
            )
            df_input[f"{item}_äºˆæ¸¬"] = model_dict["meta_model"].predict(meta_input)[0]

        stage2_input = df_input[
            [
                f"{model_dict['target_items'][0]}_äºˆæ¸¬",
                f"{model_dict['target_items'][1]}_äºˆæ¸¬",
                f"{model_dict['target_items'][2]}_äºˆæ¸¬",
                "æ›œæ—¥",
                "é€±ç•ªå·",
                "åˆè¨ˆ_å‰æ—¥",
                "1å°ã‚ãŸã‚Šæ­£å‘³é‡é‡_å‰æ—¥ä¸­å¤®å€¤",
                "ç¥æ—¥ãƒ•ãƒ©ã‚°",
            ]
        ]
        y_pred = model_dict["gbdt_model"].predict(stage2_input)[0]
        y_adjusted = y_pred + model_dict["bias"]
        lower = y_adjusted - 1.96 * model_dict["std"]
        upper = y_adjusted + 1.96 * model_dict["std"]

        label = "é€šå¸¸"
        prob = None
        if 85000 <= y_adjusted <= 95000:
            X_clf = stage2_input.drop(columns=["ç¥æ—¥ãƒ•ãƒ©ã‚°"])
            prob = model_dict["clf_model"].predict_proba(X_clf)[0][1]
            classification = model_dict["clf_model"].predict(X_clf)[0]
            label = "è­¦å‘Š" if classification == 1 else "æ³¨æ„"

        results.append(
            {
                "æ—¥ä»˜": predict_date.strftime("%Y-%m-%d"),
                "äºˆæ¸¬å€¤": y_pred,
                "è£œæ­£å¾Œäºˆæ¸¬": y_adjusted,
                "ä¸‹é™95CI": lower,
                "ä¸Šé™95CI": upper,
                "åˆ¤å®šãƒ©ãƒ™ãƒ«": label,
                "æœªæº€ç¢ºç‡": round(prob, 3) if prob is not None else None,
            }
        )

    return pd.DataFrame(results).set_index("æ—¥ä»˜")


if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
    df_raw = load_data_from_sqlite()

    #
    df_raw = load_data_from_sqlite()
    df_raw["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_raw["ä¼ç¥¨æ—¥ä»˜"])
    df_raw = df_raw[df_raw["ä¼ç¥¨æ—¥ä»˜"] >= "2024-04-01"]

    # ç¥æ—¥ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    start_date = df_raw["ä¼ç¥¨æ—¥ä»˜"].min().date()
    end_date = df_raw["ä¼ç¥¨æ—¥ä»˜"].max().date()
    holidays = get_japanese_holidays(start=start_date, end=end_date, as_str=True)

    # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    make_model = train_model_with_holiday(df_raw, holidays)

    # äºˆæ¸¬ã®å®Ÿè¡Œ
    df_pred = predict_with_model(make_model, "2025-06-01", "2025-06-07")

    print(df_pred)
