import pandas as pd
import os
from utils.get_holydays import get_japanese_holidays
from logic.factory_manage.sql import save_df_to_sqlite_unique
from utils.config_loader import get_path_from_yaml
from utils.cleaners import enforce_dtypes, strip_whitespace
from utils.config_loader import get_expected_dtypes_by_template


def make_sql_old():
    """
    éå»ã®è¤‡æ•°å¹´åˆ†ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€
    æ—¥ä»˜ã‚„æ•°å€¤æ•´å½¢ã€ä¸è¦ãƒ‡ãƒ¼ã‚¿ã®é™¤å¤–ã€ç¥æ—¥ãƒ•ãƒ©ã‚°ä»˜ä¸ã‚’è¡Œã„ã€
    SQLiteã«çµ±åˆä¿å­˜ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚
    """
    base_dir = get_path_from_yaml("input", section="directories")

    # --- å…±é€šå®šç¾© ---
    dtype_new = {"ä¼ç¥¨æ—¥ä»˜": "str", "å“å": "str", "æ­£å‘³é‡é‡": "float64"}
    dtype_old = {"ä¼ç¥¨æ—¥ä»˜": "str", "å•†å“": "str", "æ­£å‘³é‡é‡_æ˜ç´°": "float64"}
    usecols_new = ["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡", "å“å"]
    usecols_old = ["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]
    old_files = ["2020é¡§å®¢.csv", "2021é¡§å®¢.csv", "2022é¡§å®¢.csv", "2023_all.csv"]

    def load_and_clean_csv(
        filename: str, dtype: dict, usecols: list, is_old: bool
    ) -> pd.DataFrame:
        path = os.path.join(base_dir, filename)
        df = pd.read_csv(path, dtype=dtype, usecols=usecols, encoding="utf-8")

        if is_old:
            df.rename(
                columns={"å•†å“": "å“å", "æ­£å‘³é‡é‡_æ˜ç´°": "æ­£å‘³é‡é‡"}, inplace=True
            )

        # æ‹¬å¼§ä»˜ãæ—¥ä»˜ã®é™¤å»ã¨å‹å¤‰æ›
        df["ä¼ç¥¨æ—¥ä»˜"] = (
            df["ä¼ç¥¨æ—¥ä»˜"].astype(str).str.replace(r"\(.*\)", "", regex=True)
        )
        df["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")
        df["æ­£å‘³é‡é‡"] = pd.to_numeric(df["æ­£å‘³é‡é‡"], errors="coerce")

        # NaNé™¤å»
        before = len(df)
        df = df.dropna(subset=["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"])
        after_nan = len(df)
        print(f"ğŸ§¹ {filename}: NaNé™¤å» {before - after_nan}ä»¶ â†’ {after_nan}ä»¶")

        # é‡é‡0é™¤å»
        df = df[df["æ­£å‘³é‡é‡"] != 0]
        print(f"ğŸ§¹ {filename}: æ­£å‘³é‡é‡=0 é™¤å»å¾Œ {len(df)}ä»¶")

        return df

    # --- æœ€æ–°ãƒ‡ãƒ¼ã‚¿ ---
    print("ğŸ“¥ æœ€æ–°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    df_new = load_and_clean_csv(
        "20240501-20250422.csv", dtype_new, usecols_new, is_old=False
    )

    # --- éå»ãƒ‡ãƒ¼ã‚¿ ---
    print("ğŸ“¥ éå»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    df_old_list = [
        load_and_clean_csv(fname, dtype_old, usecols_old, is_old=True)
        for fname in old_files
    ]
    df_old = pd.concat(df_old_list, ignore_index=True)

    # --- çµåˆ ---
    df_raw = pd.concat([df_new, df_old], ignore_index=True)
    print(f"ğŸ“¦ ç·è¡Œæ•°ï¼ˆdf_new + df_oldï¼‰: {len(df_raw)}")

    # --- ç¥æ—¥ãƒ•ãƒ©ã‚°ä»˜ä¸ ---
    start_date = df_raw["ä¼ç¥¨æ—¥ä»˜"].min().date()
    end_date = df_raw["ä¼ç¥¨æ—¥ä»˜"].max().date()
    holidays = get_japanese_holidays(start=start_date, end=end_date, as_str=False)
    holiday_set = set(holidays)

    df_raw["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df_raw["ä¼ç¥¨æ—¥ä»˜"].dt.date.apply(
        lambda x: 1 if x in holiday_set else 0
    )
    print("ğŸŒ ç¥æ—¥ãƒ•ãƒ©ã‚°ä»˜ä¸å®Œäº†")

    # --- SQLiteä¿å­˜ ---
    try:
        db_path = get_path_from_yaml("weight_data", section="sql_database")
        save_df_to_sqlite_unique(df=df_raw, db_path=db_path, table_name="ukeire")
        print("âœ… SQLiteä¿å­˜å®Œäº†")
    except Exception as e:
        print(f"âŒ SQLiteä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    return df_raw


def make_sql_db(df: pd.DataFrame):
    """
    ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã€
    æ•´å½¢ãƒ»ç¥æ—¥ä»˜ä¸ã‚’ã—ã¦SQLiteã«ä¿å­˜ã™ã‚‹ã€‚

    Args:
        df (pd.DataFrame): å…ƒãƒ‡ãƒ¼ã‚¿
    """
    print(f"ğŸ” å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°: {len(df)}")

    df["ä¼ç¥¨æ—¥ä»˜"] = df["ä¼ç¥¨æ—¥ä»˜"].astype(str).str.replace(r"\(.*\)", "", regex=True)
    df["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")
    df["æ­£å‘³é‡é‡"] = pd.to_numeric(df["æ­£å‘³é‡é‡"], errors="coerce")

    dropped_rows = df[df[["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡"]].isna().any(axis=1)]
    dropped_rows.to_csv("dropped_rows.csv", index=False)

    df = df.dropna(subset=["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"])
    print(f"ğŸ” dropnaå¾Œã®è¡Œæ•°: {len(df)}")

    df = df[df["æ­£å‘³é‡é‡"] != 0]
    print(f"ğŸ” æ­£å‘³é‡é‡â‰ 0 ã®è¡Œæ•°: {len(df)}")

    start_date = df["ä¼ç¥¨æ—¥ä»˜"].min().date()
    end_date = df["ä¼ç¥¨æ—¥ä»˜"].max().date()
    holidays = get_japanese_holidays(start=start_date, end=end_date, as_str=False)
    holiday_set = set(holidays)
    df["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df["ä¼ç¥¨æ—¥ä»˜"].dt.date.apply(
        lambda x: 1 if x in holiday_set else 0
    )

    df = df.loc[:, ["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡", "å“å", "ç¥æ—¥ãƒ•ãƒ©ã‚°"]]
    print(f"ğŸ” æ•´å½¢å¾Œã®è¡Œæ•°: {len(df)}")

    try:
        db_path = get_path_from_yaml("weight_data", section="sql_database")
        save_df_to_sqlite_unique(
            df=df,
            db_path=db_path,
            table_name="ukeire",
        )
    except Exception as e:
        print(f"âŒ SQLiteä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def make_csv(df: pd.DataFrame) -> pd.DataFrame:
    """
    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«åŸºã¥ã„ã¦å—å…¥ãƒ‡ãƒ¼ã‚¿ã®ç©ºç™½é™¤å»ãƒ»å‹å¤‰æ›ã‚’è¡Œã†ã€‚

    Args:
        df (pd.DataFrame): å…ƒãƒ‡ãƒ¼ã‚¿

    Returns:
        pd.DataFrame: æ•´å½¢æ¸ˆãƒ‡ãƒ¼ã‚¿
    """
    df = strip_whitespace(df)
    expected_dtypes = get_expected_dtypes_by_template("inbound_volume")
    dtypes = expected_dtypes.get("receive", {})

    if dtypes:
        df = enforce_dtypes(df, dtypes)

    return df


# --- å®Ÿè¡Œ ---
if __name__ == "__main__":
    make_sql_old()
