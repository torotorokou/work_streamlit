import pandas as pd
from utils.get_holydays import get_japanese_holidays
from logic.factory_manage.sql import save_df_to_sqlite_unique
from utils.config_loader import get_path_from_yaml
from utils.cleaners import enforce_dtypes, strip_whitespace
from utils.config_loader import get_expected_dtypes_by_template


def make_df_old():
    # --- å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã®å–å¾— ---
    base_dir = get_path_from_yaml("input", section="directories")

    # --- å…±é€šå®šç¾© ---
    dtype_new = {"ä¼ç¥¨æ—¥ä»˜": "str", "å“å": "str", "æ­£å‘³é‡é‡": "float64"}
    dtype_old = {"ä¼ç¥¨æ—¥ä»˜": "str", "å•†å“": "str", "æ­£å‘³é‡é‡_æ˜ç´°": "float64"}
    usecols_new = ["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡", "å“å"]
    usecols_old = ["ä¼ç¥¨æ—¥ä»˜", "å•†å“", "æ­£å‘³é‡é‡_æ˜ç´°"]

    # --- CSVèª­ã¿è¾¼ã¿ç”¨é–¢æ•° ---
    def load_csv(filename: str, dtype: dict, usecols: list) -> pd.DataFrame:
        return pd.read_csv(
            f"{base_dir}/{filename}", dtype=dtype, usecols=usecols, encoding="utf-8"
        )

    # --- æ–°ãƒ‡ãƒ¼ã‚¿ ---
    df_new = load_csv("20240501-20250422.csv", dtype=dtype_new, usecols=usecols_new)

    # --- æ—§ãƒ‡ãƒ¼ã‚¿ï¼ˆè¤‡æ•°å¹´ï¼‰ ---
    old_files = ["2020é¡§å®¢.csv", "2021é¡§å®¢.csv", "2022é¡§å®¢.csv", "2023_all.csv"]
    df_old_list = [
        load_csv(fname, dtype=dtype_old, usecols=usecols_old) for fname in old_files
    ]
    df_old = pd.concat(df_old_list, ignore_index=True)
    df_old.rename(columns={"å•†å“": "å“å", "æ­£å‘³é‡é‡_æ˜ç´°": "æ­£å‘³é‡é‡"}, inplace=True)

    # --- df_old æ•´å½¢ ---
    print(f"ğŸ“„ df_old åŸå§‹è¡Œæ•°: {len(df_old)}")
    df_old["ä¼ç¥¨æ—¥ä»˜"] = (
        df_old["ä¼ç¥¨æ—¥ä»˜"].astype(str).str.replace(r"\(.*\)", "", regex=True)
    )
    df_old["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_old["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")
    df_old["æ­£å‘³é‡é‡"] = pd.to_numeric(df_old["æ­£å‘³é‡é‡"], errors="coerce")
    old_nan_dropped = df_old[df_old[["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"]].isna().any(axis=1)]
    print(f"ğŸ—‘ df_old NaNã§å‰Šé™¤: {len(old_nan_dropped)} è¡Œ")
    df_old = df_old.dropna(subset=["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"])
    old_zero_dropped = df_old[df_old["æ­£å‘³é‡é‡"] == 0]
    print(f"ğŸ—‘ df_old æ­£å‘³é‡é‡=0ã§å‰Šé™¤: {len(old_zero_dropped)} è¡Œ")
    df_old = df_old[df_old["æ­£å‘³é‡é‡"] != 0]

    # --- df_new æ•´å½¢ ---
    print(f"ğŸ“„ df_new åŸå§‹è¡Œæ•°: {len(df_new)}")
    df_new["ä¼ç¥¨æ—¥ä»˜"] = (
        df_new["ä¼ç¥¨æ—¥ä»˜"].astype(str).str.replace(r"\(.*\)", "", regex=True)
    )
    df_new["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df_new["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")
    df_new["æ­£å‘³é‡é‡"] = pd.to_numeric(df_new["æ­£å‘³é‡é‡"], errors="coerce")
    new_nan_dropped = df_new[df_new[["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"]].isna().any(axis=1)]
    print(f"ğŸ—‘ df_new NaNã§å‰Šé™¤: {len(new_nan_dropped)} è¡Œ")
    df_new = df_new.dropna(subset=["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"])
    new_zero_dropped = df_new[df_new["æ­£å‘³é‡é‡"] == 0]
    print(f"ğŸ—‘ df_new æ­£å‘³é‡é‡=0ã§å‰Šé™¤: {len(new_zero_dropped)} è¡Œ")
    df_new = df_new[df_new["æ­£å‘³é‡é‡"] != 0]

    # --- çµåˆ ---
    df_raw = pd.concat([df_new, df_old], ignore_index=True)
    print(f"ğŸ“¦ çµåˆå¾Œã®ç·è¡Œæ•°: {len(df_raw)}")

    # --- ç¥æ—¥ãƒ•ãƒ©ã‚°è¿½åŠ  ---
    start_date = df_raw["ä¼ç¥¨æ—¥ä»˜"].min().date()
    end_date = df_raw["ä¼ç¥¨æ—¥ä»˜"].max().date()
    holidays = get_japanese_holidays(start=start_date, end=end_date, as_str=False)
    holiday_set = set(holidays)
    df_raw["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df_raw["ä¼ç¥¨æ—¥ä»˜"].dt.date.apply(
        lambda x: 1 if x in holiday_set else 0
    )

    # --- SQLiteä¿å­˜ ---
    try:
        db_path = get_path_from_yaml("weight_data", section="sql_database")
        save_df_to_sqlite_unique(df=df_raw, db_path=db_path, table_name="ukeire")
        print("âœ… SQLiteä¿å­˜å®Œäº†")
    except Exception as e:
        print(f"âŒ SQLiteä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def make_sql_db(df: pd.DataFrame):
    print(f"ğŸ” å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°: {len(df)}")

    # --- æ—¥ä»˜åˆ—ã®æ•´å½¢ï¼ˆæ›œæ—¥ãªã©ã‚’é™¤å»ï¼‰ ---
    df["ä¼ç¥¨æ—¥ä»˜"] = df["ä¼ç¥¨æ—¥ä»˜"].astype(str).str.replace(r"\(.*\)", "", regex=True)
    df["ä¼ç¥¨æ—¥ä»˜"] = pd.to_datetime(df["ä¼ç¥¨æ—¥ä»˜"], errors="coerce")

    # --- æ•°å€¤å¤‰æ›ï¼ˆæ­£å‘³é‡é‡ï¼‰ ---
    df["æ­£å‘³é‡é‡"] = pd.to_numeric(df["æ­£å‘³é‡é‡"], errors="coerce")

    # --- æ¬ æè¡Œã®ä¿å­˜ ---
    dropped_rows = df[df[["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡"]].isna().any(axis=1)]
    dropped_rows.to_csv("dropped_rows.csv", index=False)

    # --- æ¬ æé™¤å» ---
    df = df.dropna(subset=["æ­£å‘³é‡é‡", "ä¼ç¥¨æ—¥ä»˜"])
    print(f"ğŸ” dropnaå¾Œã®è¡Œæ•°: {len(df)}")

    # --- æ­£å‘³é‡é‡ãŒ0ã®è¡Œã‚’å‰Šé™¤ ---
    df = df[df["æ­£å‘³é‡é‡"] != 0]
    print(f"ğŸ” æ­£å‘³é‡é‡â‰ 0 ã®è¡Œæ•°: {len(df)}")

    # --- ç¥æ—¥ãƒ•ãƒ©ã‚°è¿½åŠ  ---
    start_date = df["ä¼ç¥¨æ—¥ä»˜"].min().date()
    end_date = df["ä¼ç¥¨æ—¥ä»˜"].max().date()
    holidays = get_japanese_holidays(start=start_date, end=end_date, as_str=False)
    holiday_set = set(holidays)
    df["ç¥æ—¥ãƒ•ãƒ©ã‚°"] = df["ä¼ç¥¨æ—¥ä»˜"].dt.date.apply(
        lambda x: 1 if x in holiday_set else 0
    )
    print(f"ğŸ” ç¥æ—¥ãƒ•ãƒ©ã‚°è¿½åŠ å¾Œã®è¡Œæ•°: {len(df)}")

    # --- å¿…è¦åˆ—ã«é™å®š ---
    df = df.loc[:, ["ä¼ç¥¨æ—¥ä»˜", "æ­£å‘³é‡é‡", "å“å", "ç¥æ—¥ãƒ•ãƒ©ã‚°"]]
    print(f"ğŸ” æ•´å½¢å¾Œã®è¡Œæ•°: {len(df)}")

    # --- SQLiteä¿å­˜ ---
    try:
        db_path = get_path_from_yaml("weight_data", section="sql_database")
        save_df_to_sqlite_unique(
            df=df,
            db_path=db_path,
            table_name="ukeire",
        )
    except Exception as e:
        print(f"âŒ SQLiteä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def make_csv(df):
    # ç©ºç™½é™¤å»
    df = strip_whitespace(df)

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«åŸºã¥ãå‹å®šç¾©ã‚’å–å¾—
    expected_dtypes = get_expected_dtypes_by_template("inbound_volume")
    dtypes = expected_dtypes.get("receive", {})

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’é©ç”¨
    if dtypes:
        df = enforce_dtypes(df, dtypes)

    return df


# å®Ÿè¡Œ
if __name__ == "__main__":
    make_df_old()
