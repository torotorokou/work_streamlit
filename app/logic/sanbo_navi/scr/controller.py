# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿ ---
import streamlit as st
from langchain_community.vectorstores import FAISS

from logic.sanbo_navi.scr.view import load_pdf_first_page

from logic.sanbo_navi.scr.loader import (
    load_config,
    load_json_data,
    extract_categories_and_titles,
    load_question_templates,
)

# AIé–¢é€£
from logic.sanbo_navi.scr.ai_loader import OpenAIConfig
from logic.sanbo_navi.scr.ai_loader import load_ai
from logic.sanbo_navi.scr.llm_utils import OpenAIClient
from logic.sanbo_navi.scr.llm_utils import suggest_category, generate_answer
from logic.sanbo_navi.scr.utils import load_vectorstore

# css
from components.custom_button import centered_button
from logic.sanbo_navi.scr.view import render_pdf_pages


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def contoroller_education_gpt_page():
    # =============================
    # å„ç¨®è¨­å®šãƒ»ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    # =============================
    FAISS_PATH, PDF_PATH, JSON_PATH = load_config()
    json_data = load_json_data(JSON_PATH)
    categories, _ = extract_categories_and_titles(json_data)
    templates = load_question_templates()

    # =============================
    # AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
    # =============================
    client = load_ai(OpenAIConfig)
    llm_client = OpenAIClient(client)  # LLMClientBase ç¶™æ‰¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    vectorstore = load_vectorstore(api_key=client.api_key, FAISS_PATH=FAISS_PATH)

    # =============================
    # UI æ§‹ç¯‰
    # =============================
    st.title("ğŸ“˜ æ•™è‚²GPTã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    st.markdown(
        "SOLVESTã«ã¤ã„ã¦è³ªå•ã§ãã¾ã™ã€‚ã¾ãšæœ€åˆã«çŸ¥ã‚ŠãŸã„ã“ã¨ã‚’ä¸€è¨€ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    )

    # --- PDF ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ---
    with st.expander("ğŸ“„ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ã®ã¿å…ˆã«è¡¨ç¤ºï¼‰"):
        pdf_first_page = load_pdf_first_page(PDF_PATH)
        st.image(pdf_first_page[0], caption="Page 1", use_container_width=True)

    # --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•å…¥åŠ› ---
    query_input = st.text_input(
        "çŸ¥ã‚ŠãŸã„ã“ã¨ï¼ˆä¾‹ï¼šå…­é¢æ¢±åŒ…æ©Ÿã®å‡¦ç†èƒ½åŠ›ã€è¨±èªå¯ã®å–å¾—æ–¹æ³• ãªã©ï¼‰"
    )

    # =============================
    # ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•ææ¡ˆãƒœã‚¿ãƒ³
    # =============================
    suggested_category = None
    if centered_button("â¡ï¸ ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•ææ¡ˆ"):
        if query_input.strip():
            with st.spinner("ğŸ¤– ã‚«ãƒ†ã‚´ãƒªã‚’ææ¡ˆä¸­..."):
                suggested_category = suggest_category(query_input, llm_client)
                st.session_state.suggested_category = suggested_category
        else:
            st.warning("ã¾ãšçŸ¥ã‚ŠãŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # =============================
    # ã‚«ãƒ†ã‚´ãƒªé¸æŠ UI
    # =============================
    default_category = st.session_state.get("suggested_category", None)
    main_category = st.selectbox(
        "ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        options=categories,
        index=(
            categories.index(default_category) if default_category in categories else 0
        ),
    )

    # =============================
    # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªé¸æŠ or è‡ªç”±å…¥åŠ›
    # =============================
    category_template = templates.get(main_category, templates["default"])
    subcategory_options = ["è‡ªç”±å…¥åŠ›"] + category_template
    sub_category = st.selectbox("å¯¾è±¡ã‚’é¸æŠ", options=subcategory_options)

    # --- è³ªå•æ–‡ã®çµ„ã¿ç«‹ã¦ ---
    if sub_category == "è‡ªç”±å…¥åŠ›":
        user_input = st.text_area("ã•ã‚‰ã«è©³ã—ãè³ªå•ã‚’å…¥åŠ›", height=100)
        query = user_input if user_input.strip() else query_input
    else:
        query = (
            f"{query_input} ã«é–¢ã—ã¦ã€{sub_category} ã‚’è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"
            if query_input.strip()
            else f"{sub_category} ã«é–¢ã—ã¦ã€è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"
        )

    # =============================
    # å›ç­”ç”Ÿæˆãƒœã‚¿ãƒ³
    # =============================
    if centered_button("â¡ï¸ é€ä¿¡") and query.strip():
        with st.spinner("ğŸ¤– å›ç­”ç”Ÿæˆä¸­..."):
            answer, sources = generate_answer(
                query, main_category, vectorstore, llm_client
            )
            st.session_state.last_response = answer
            st.session_state.sources = sources

    # =============================
    # å›ç­”è¡¨ç¤º + å‡ºå…¸è¡¨ç¤º
    # =============================
    if "last_response" in st.session_state:
        st.success("âœ… å›ç­”")
        st.markdown(st.session_state.last_response)

        if "sources" in st.session_state:
            if "cache_pdf_pages" not in st.session_state:
                st.session_state.cache_pdf_pages = {}

            # --- ãƒšãƒ¼ã‚¸ã®æŠ½å‡ºï¼ˆæ•´æ•° or "3-5" ãªã©ã®æ–‡å­—åˆ—ã‚’è¨±å®¹ï¼‰ ---
            pages = set()
            for _, p in st.session_state.sources:
                if p is None:
                    continue
                p_str = str(p).strip()
                if p_str.isdigit() or "-" in p_str:
                    pages.add(p_str)

            # è¡¨ç¤ºç”¨ã®æ–‡å­—åˆ—
            st.markdown(
                "ğŸ“„ **å‡ºå…¸ãƒšãƒ¼ã‚¸:** " + ", ".join([f"Page {p}" for p in sorted(pages)])
            )

            # è¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œé–¢æ•°ã«æ¸¡ã™
            render_pdf_pages(PDF_PATH, pages)


if __name__ == "__main__":
    contoroller_education_gpt_page()
