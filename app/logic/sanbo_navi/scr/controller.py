# --- ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿ ---
import streamlit as st
import json
from dotenv import load_dotenv
from openai import OpenAI
from pdf2image import convert_from_path
from PIL import Image
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from logic.sanbo_navi.scr.view import load_pdf_first_page, load_pdf_page


from logic.sanbo_navi.scr.loader import (
    load_config,
    load_json_data,
    extract_categories_and_titles,
)
from logic.sanbo_navi.scr.loader import load_question_templates
from logic.sanbo_navi.scr.loader import get_resource_paths

# AIè¨­å®š
from logic.sanbo_navi.scr.ai_loader import OpenAIConfig
from logic.sanbo_navi.scr.ai_loader import load_ai

# å¤–éƒ¨èª­è¾¼
from logic.sanbo_navi.scr.loader import get_resource_paths


# --- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®èª­ã¿è¾¼ã¿ ---
@st.cache_resource
def load_vectorstore(api_key: str = None, FAISS_PATH: str = None):
    # FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ­ãƒ¼ãƒ‰
    if not api_key:
        st.warning("OPENAI_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚")
        return None
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    return FAISS.load_local(
        FAISS_PATH, embeddings=embeddings, allow_dangerous_deserialization=True
    )


def contoroller_education_gpt_page():
    # st.title("ğŸ“˜ æ•™è‚²GPTã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    # --- è¨­å®šã®èª­ã¿è¾¼ã¿ ---
    FAISS_PATH, PDF_PATH, JSON_PATH = load_config()

    #  AIã®è¨­å®šã‚’èª­è¾¼
    client = load_ai(OpenAIConfig)  # OpenAIConfigã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¯ãƒ©ã‚¹ã¨ã—ã¦ä½¿ç”¨

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®èª­è¾¼
    vectorstore = load_vectorstore(api_key=client.api_key, FAISS_PATH=FAISS_PATH)

    # --- JSONãƒ‡ãƒ¼ã‚¿ã®èª­è¾¼ ---
    json_data = load_json_data(JSON_PATH)
    categories, subcategory_map = extract_categories_and_titles(json_data)

    # --- ã‚«ãƒ†ã‚´ãƒªsuggestionç”¨é–¢æ•° ---
    def suggest_category(query_input: str) -> str:
        # GPTã‚’ä½¿ã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰æœ€é©ãªã‚«ãƒ†ã‚´ãƒªã‚’ææ¡ˆã™ã‚‹
        prompt = f"""
    ä»¥ä¸‹ã¯ã€æ—¥æœ¬ã®ç”£æ¥­å»ƒæ£„ç‰©å‡¦ç†ã«é–¢ã™ã‚‹æ•™è‚²ç”¨AIã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ã‚ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªã®ä¸€è¦§ã§ã™ï¼š
    
    - å‡¦ç†å·¥ç¨‹
    - è¨­å‚™
    - è¡Œæ”¿ãƒ»è¨±èªå¯
    - ç”£å»ƒåˆ†é¡ãƒ»å“ç›®
    - æ–½è¨­
    
    æ¬¡ã®è³ªå•ã¯ã€ã©ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹ã®ãŒã‚‚ã£ã¨ã‚‚é©åˆ‡ã§ã—ã‚‡ã†ã‹ï¼Ÿ
    ã‚«ãƒ†ã‚´ãƒªåã‚’1ã¤ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆãã‚Œä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ï¼‰
    
    ã€è³ªå•ã€‘  
    {query_input}
    """
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "ã‚ãªãŸã¯æ—¥æœ¬ã®ç”£æ¥­å»ƒæ£„ç‰©å‡¦ç†ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã«è©³ã—ã„å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
                },
                {"role": "user", "content": prompt},
            ],
        )
        suggestion = response.choices[0].message.content.strip()
        return suggestion

    # --- UIæ§‹ç¯‰ ---
    # --- ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜ ---
    st.title("ğŸ“˜ æ•™è‚²GPTã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
    st.markdown(
        "SOLVESTã«ã¤ã„ã¦è³ªå•ã§ãã¾ã™ã€‚ã¾ãšæœ€åˆã«çŸ¥ã‚ŠãŸã„ã“ã¨ã‚’ä¸€è¨€ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    )

    # --- PDF 1ãƒšãƒ¼ã‚¸ç›® ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ---
    with st.expander("ğŸ“„ PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆ1ãƒšãƒ¼ã‚¸ç›®ã®ã¿å…ˆã«è¡¨ç¤ºï¼‰"):
        pdf_first_page = load_pdf_first_page(PDF_PATH)
        st.image(pdf_first_page[0], caption="Page 1", use_column_width=True)

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘  ãƒ¦ãƒ¼ã‚¶ãƒ¼è‡ªç”±å…¥åŠ› ---
    query_input = st.text_input(
        "çŸ¥ã‚ŠãŸã„ã“ã¨ï¼ˆä¾‹ï¼šå…­é¢æ¢±åŒ…æ©Ÿã®å‡¦ç†èƒ½åŠ›ã€è¨±èªå¯ã®å–å¾—æ–¹æ³• ãªã©ï¼‰"
    )

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘¡ ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•ææ¡ˆ ---
    suggested_category = None
    if st.button("â¡ï¸ ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•ææ¡ˆ"):
        if query_input.strip():
            with st.spinner("ğŸ¤– ã‚«ãƒ†ã‚´ãƒªã‚’ææ¡ˆä¸­..."):
                suggested_category = suggest_category(query_input)
                st.session_state.suggested_category = suggested_category
        else:
            st.warning("ã¾ãšçŸ¥ã‚ŠãŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘¢ ã‚«ãƒ†ã‚´ãƒªé¸æŠbox ---
    default_category = st.session_state.get("suggested_category", None)
    main_category = st.selectbox(
        "ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ",
        options=categories,
        index=(
            categories.index(default_category) if default_category in categories else 0
        ),
    )

    # --- ã‚¹ãƒ†ãƒƒãƒ—â‘£ å¯¾è±¡é¸æŠ ---
    # è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ­ãƒ¼ãƒ‰
    templates = load_question_templates()
    category_template = templates.get(main_category, templates["default"])

    # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªé¸æŠè‚¢ã®ç”Ÿæˆ
    subcategory_options = ["è‡ªç”±å…¥åŠ›"] + templates.get(
        main_category, templates["default"]
    )
    sub_category = st.selectbox("å¯¾è±¡ã‚’é¸æŠ", options=subcategory_options)

    # --- è³ªå•æ–‡æ§‹ç¯‰ ---
    if sub_category == "è‡ªç”±å…¥åŠ›":
        user_input = st.text_area("ã•ã‚‰ã«è©³ã—ãè³ªå•ã‚’å…¥åŠ›", height=100)
        query = user_input if user_input.strip() else query_input
    else:
        query = (
            f"{query_input} ã«é–¢ã—ã¦ã€{sub_category} ã‚’è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"
            if query_input.strip()
            else f"{sub_category} ã«é–¢ã—ã¦ã€è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"
        )

    # --- å›ç­”ç”Ÿæˆ ---
    def generate_answer(query: str, selected_category: str):
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ¤œç´¢ã§é–¢é€£æ–‡æ›¸å–å¾—
        docs = vectorstore.max_marginal_relevance_search(query, k=5, fetch_k=30)

        # é¸æŠã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚‹å ´åˆã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if selected_category:
            docs = [
                doc
                for doc in docs
                if selected_category in doc.metadata.get("category", [])
            ]

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        context = "\n".join([doc.page_content for doc in docs])
        sources = [
            (doc.metadata.get("source", "ä¸æ˜"), doc.metadata.get("page", "ä¸æ˜"))
            for doc in docs
        ]

        # å›ç­”ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""
    ä»¥ä¸‹ã¯ã€ç”£æ¥­å»ƒæ£„ç‰©ã®å‡¦ç†ãƒ»è¨­å‚™ãƒ»å·¥ç¨‹ã«é–¢ã™ã‚‹æ§‹é€ åŒ–æŠ€è¡“æ–‡æ›¸ã®æŠœç²‹ã§ã™ã€‚
    ã“ã®æŠœç²‹ã«åŸºã¥ãã€ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ **ã§ãã‚‹é™ã‚Šæ–‡æ›¸å†…å®¹ã‚’å„ªå…ˆã—ãªãŒã‚‰ã€å·¥ç¨‹é †ãƒ»è«–ç†é †ã«æ²¿ã£ã¦æ­£ç¢ºã‹ã¤å®Ÿå‹™çš„ã«**ç­”ãˆã¦ãã ã•ã„ã€‚
    
    ã€è³‡æ–™æŠœç²‹ã€‘
    {context}
    
    ã€è³ªå•ã€‘
    {query}
    
    ### ã€å›ç­”ãƒ«ãƒ¼ãƒ«ã€‘
    1. è³‡æ–™ã®è¨˜è¼‰ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
    2. æ˜è¨˜ãŒãªã„å ´åˆã¯ä¸€èˆ¬çš„è¦‹è§£ã§è£œè¶³ã—ã¦ãã ã•ã„ã€‚
    3. å›ç­”ã¯ 1. 2. 3. ã®ã‚ˆã†ã«ç®‡æ¡æ›¸ãã§ã€‚
    4. ä¿¡é ¼åº¦ã‚„å‡ºå…¸ï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ï¼‰ã‚‚å¯èƒ½ãªé™ã‚Šæ˜ç¤ºã—ã¦ãã ã•ã„ã€‚
    """
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "ã‚ãªãŸã¯æ—¥æœ¬ã®ç”£æ¥­å»ƒæ£„ç‰©å‡¦ç†æ–‡æ›¸ã‚’ã‚‚ã¨ã«å›ç­”ã™ã‚‹å°‚é–€AIã§ã™ã€‚",
                },
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content, sources

    # --- å›ç­”è¡¨ç¤º ---
    if st.button("â¡ï¸ é€ä¿¡") and query.strip():
        with st.spinner("ğŸ¤– å›ç­”ç”Ÿæˆä¸­..."):
            answer, sources = generate_answer(query, main_category)
            st.session_state.last_response = answer
            st.session_state.sources = sources

    if "last_response" in st.session_state:
        st.success("âœ… å›ç­”")
        st.markdown(st.session_state.last_response)

        if "sources" in st.session_state:
            # PDFãƒšãƒ¼ã‚¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆæœŸåŒ–
            if "cache_pdf_pages" not in st.session_state:
                st.session_state.cache_pdf_pages = {}

            # å‡ºå…¸ãƒšãƒ¼ã‚¸ç•ªå·ã®æŠ½å‡º
            pages = set(int(p) for _, p in st.session_state.sources if str(p).isdigit())
            st.markdown("ğŸ“„ **å‡ºå…¸ãƒšãƒ¼ã‚¸:** " + ", ".join([f"Page {p}" for p in pages]))

            # å‡ºå…¸ãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
            with st.expander("ğŸ“˜ å‡ºå…¸ãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                for p in sorted(pages):
                    if isinstance(p, int) and p >= 1:
                        if p in st.session_state.cache_pdf_pages:
                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
                            st.image(
                                st.session_state.cache_pdf_pages[p],
                                caption=f"Page {p} (cached)",
                                use_column_width=True,
                            )
                        else:
                            # PDFã‹ã‚‰èª­ã¿è¾¼ã¿
                            with st.spinner(f"ğŸ“„ Page {p} èª­ã¿è¾¼ã¿ä¸­..."):
                                page_image = load_pdf_page(PDF_PATH, p)
                                st.session_state.cache_pdf_pages[p] = page_image
                                st.image(
                                    page_image,
                                    caption=f"Page {p}",
                                    use_column_width=True,
                                )


if __name__ == "__main__":
    contoroller_education_gpt_page()
